{'input': 'Data-Efiecient Structured Pruning via Submodular Optimization\nMarwa El Halabi∗1Suraj Srinivas2Simon Lacoste-Julien1,3\n1Samsung, SAIT AI Lab, Montreal\n2Idiap Research Institute & EPFL\n3Mila, Universit\x13 e de Montreal, Canada CIFAR AI Chair\nAbstract\nStructured pruning is an e\x0bective approach for compressing large pre-trained neural networks\nwithout signi\x0ccantly a\x0becting their performance, which involves removing redundant regular\nregions of weights', 'question': 'What is structured pruning and how does it help compress large pre-trained neural networks?', 'ans': 'Structured pruning is an effective approach for compressing large pre-trained neural networks without significantly affecting their performance. It involves removing redundant regular regions of weights.'}, {'input': 'However, current structured pruning methods are highly empirical in nature,\ndo not provide any theoretical guarantees, and often require \x0cne-tuning, which makes them\ninapplicable in the limited-data regime', 'question': 'What are the limitations of current structured pruning methods?', 'ans': 'Current structured pruning methods are highly empirical in nature, do not provide any theoretical guarantees, and often require fine-tuning, which makes them inapplicable in the limited-data regime.'}, {'input': 'However, current structured pruning methods are highly empirical in nature,\ndo not provide any theoretical guarantees, and often require \x0cne-tuning, which makes them\ninapplicable in the limited-data regime', 'question': 'What is the proposed method for structured pruning based on?', 'ans': 'The proposed method for structured pruning is based on submodular optimization.'}, {'input': 'Our method is one of the few in the literature that uses only a\nlimited-number of training data and no labels. Our experimental results demonstrate that our\nmethod outperforms popular baseline methods in various one-shot pruning settings', 'question': 'What is the method proposed in the literature that uses only a limited number of training data and no labels?', 'ans': 'The method proposed in the literature that uses only a limited number of training data and no labels is the weakly submodular maximization problem, which can be approximated using an efficient greedy algorithm.'}, {'input': 'Our method is one of the few in the literature that uses only a\nlimited-number of training data and no labels. Our experimental results demonstrate that our\nmethod outperforms popular baseline methods in various one-shot pruning settings', 'question': 'What is unique about the method described in the text?', 'ans': 'The method uses only a limited number of training data and no labels.'}, {'input': '1 Introduction\nAs modern neural networks (NN) grow increasingly large, with some models reaching billions of\nparameters (McGu\x0ee and Newhouse, 2020), they require an increasingly large amount of memory,\npower, hardware, and inference time, which makes it necessary to compress them', 'question': 'Why is it necessary to compress modern neural networks?', 'ans': 'Modern neural networks require an increasingly large amount of memory, power, hardware, and inference time as they grow larger, which makes it necessary to compress them.'}, {'input': 'This is especially\nimportant for models deployed on resource-constrained devices like mobile phones and smart speakers,\nand for latency-critical applications such as self-driving cars', 'question': 'What is the significance of deploying models on resource-constrained devices?', 'ans': 'It is especially important for models deployed on resource-constrained devices like mobile phones and smart speakers.'}, {'input': 'Fortunately, the literature on network\npruning has observed that parameters of deep NNs have a lot of redundancy, and thus they can be\ncompressed without signi\x0ccantly a\x0becting performance', 'question': 'What has the literature on network pruning observed about parameters of deep NNs?', 'ans': 'The literature on network pruning has observed that parameters of deep NNs have a lot of redundancy, and thus they can be compressed without significantly affecting performance.'}, {'input': '1 Introduction\nAs modern neural networks (NN) grow increasingly large, with some models reaching billions of\nparameters (McGu\x0ee and Newhouse, 2020), they require an increasingly large amount of memory,\npower, hardware, and inference time, which makes it necessary to compress them', 'question': 'What is the objective of compressing NNs?', 'ans': 'The objective of compressing NNs is to reduce parameter count or decrease inference time.'}, {'input': 'One family of methods approximate model weights using quantization\nor hashing (Gong et al., 2014; Courbariaux et al., 2015), while another set of methods use low-rank\napproximation or tensor factorization (Denil et al., 2013; Sainath et al., 2013; Jaderberg et al., 2014;\nDenton et al', 'question': 'What are some methods used to approximate model weights?', 'ans': 'Some methods used to approximate model weights are quantization, hashing, low-rank approximation, and tensor factorization.'}, {'input': ', 2014; Courbariaux et al., 2015), while another set of methods use low-rank\napproximation or tensor factorization (Denil et al., 2013; Sainath et al., 2013; Jaderberg et al., 2014;\nDenton et al., 2014; Lebedev et al., 2015; Kim et al., 2015; Zhang et al., 2016; Su et al., 2018)', 'question': 'What are some methods that use low-rank approximation or tensor factorization?', 'ans': 'Denil et al., 2013; Sainath et al., 2013; Jaderberg et al., 2014; Denton et al., 2014; Lebedev et al., 2015; Kim et al., 2015; Zhang et al., 2016; Su et al., 2018'}, {'input': 'Distilling the knowledge in a neural network. Neural\nInformation Processing Systems (NeurIPS) Workshops , 2015.\nTorsten Hoe\rer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in\ndeep learning: Pruning and growth for e\x0ecient inference and training in neural networks', 'question': 'What is knowledge distillation in the context of neural networks?', 'ans': 'In knowledge distillation, a small network is trained to mimic a much larger network.'}, {'input': ', 2015; Kim et al., 2015; Zhang et al., 2016; Su et al., 2018).\nIn another class of methods called knowledge distillation, a small network is trained to mimic a\nmuch larger network (Bucila et al., 2006; Hinton et al., 2015)', 'question': 'What is knowledge distillation?', 'ans': 'Knowledge distillation is a class of methods where a small network is trained to mimic a much larger network.'}, {'input': ', 2006; Hinton et al., 2015). Other methods employ sparsity and\ngroup-sparsity regularisation during training, to induce sparse weights (Collins and Kohli, 2014;\nVoita et al., 2019)', 'question': 'What are some methods that can be used to induce sparse weights during training?', 'ans': 'Some methods that can be used to induce sparse weights during training are L1 and L2 regularization, sparsity and group-sparsity regularization.'}, {'input': ', 2019). In this work, we follow the network pruning approach, where the redundant\nunits (weights, neurons or \x0clters/channels) of a pre-trained NN are removed; see (Kuzmin et al.,\n∗part of this work was done while a postdoc at MIT, CSAIL.\n1arXiv:2203.04940v1  [cs.LG]  9 Mar 20222019; Blalock et al', 'question': 'What is the network pruning approach?', 'ans': 'The network pruning approach involves removing redundant units (weights, neurons or filters/channels) of a pre-trained NN.'}, {'input': 'LG]  9 Mar 20222019; Blalock et al., 2020; Hoe\rer et al., 2021) for recent surveys. It is also possible to combine two\nor more of these types of compression methods to compound their bene\x0cts, see e.g., (Kuzmin et al.,\n2019, Section 4.3.4)', 'question': 'What is the purpose of combining two or more types of compression methods?', 'ans': 'To compound their benefits.'}, {'input': 'LG]  9 Mar 20222019; Blalock et al., 2020; Hoe\rer et al., 2021) for recent surveys. It is also possible to combine two\nor more of these types of compression methods to compound their bene\x0cts, see e.g., (Kuzmin et al.,\n2019, Section 4.3.4)', 'question': 'What is the purpose of combining two or more types of compression methods?', 'ans': 'To compound their benefits.'}, {'input': ', 1993). Existing pruning methods fall into two main categories: unstructured pruning methods\nwhich prune individual weights leading to irregular sparsity patterns, and structured pruning\nmethods which prune regular regions of weights, such as neurons, channels, or attention heads', 'question': 'What are pruning methods and when did they originate?', 'ans': 'Pruning methods are a type of compression method and they date back to at least the 1990s with the works of LeCun et al. and Hassibi et al.'}, {'input': ', 1993). Existing pruning methods fall into two main categories: unstructured pruning methods\nwhich prune individual weights leading to irregular sparsity patterns, and structured pruning\nmethods which prune regular regions of weights, such as neurons, channels, or attention heads', 'question': 'What are the two main categories of pruning methods?', 'ans': 'The two main categories of pruning methods are unstructured pruning methods and structured pruning methods.'}, {'input': 'Structured pruning methods are generally preferable as the resulting pruned models can work with\no\x0b-the-shelf hardware or kernels, as opposed to models pruned with unstructured pruning which\nrequire specialized ones', 'question': 'What is the advantage of using structured pruning methods?', 'ans': 'The resulting pruned models can work with off-the-shelf hardware or kernels.'}, {'input': 'Related work The goal in structured pruning is to select groups of parameters to prune that will\nminimize the decrease in performance. As minimizing the true decrease in accuracy is intractable,\none has to replace it with a proxy more amenable to e\x0ecient optimization', 'question': 'What is the goal in structured pruning?', 'ans': 'The goal in structured pruning is to select groups of parameters to prune that will minimize the decrease in performance.'}, {'input': 'As minimizing the true decrease in accuracy is intractable,\none has to replace it with a proxy more amenable to e\x0ecient optimization. Various proxy choices\nhas been proposed in the literature. Some works prune neurons/channels individually based on\ntheir input or output weights norm (He et al', 'question': 'What is the reason for replacing true decrease in accuracy with a proxy?', 'ans': 'Minimizing the true decrease in accuracy is intractable, so it has to be replaced with a proxy more amenable to efficient optimization.'}, {'input': 'Various proxy choices\nhas been proposed in the literature. Some works prune neurons/channels individually based on\ntheir input or output weights norm (He et al., 2014; Li et al., 2017), or activations entropy (He\net al., 2014), or product of activations and gradients of the loss w.r', 'question': 'What are some of the proxy choices proposed in the literature?', 'ans': 'Some of the proxy choices proposed in the literature include pruning neurons/channels based on their input or output weights norm, activations entropy, or product of activations and gradients of the loss w.r'}, {'input': ', 1993). Existing pruning methods fall into two main categories: unstructured pruning methods\nwhich prune individual weights leading to irregular sparsity patterns, and structured pruning\nmethods which prune regular regions of weights, such as neurons, channels, or attention heads', 'question': 'What are some methods used to prune neural networks?', 'ans': 'Some methods used to prune neural networks include weight magnitude, activations entropy, product of activations and gradients of the loss w.r.t the activations, and similarity of their weights to other neurons in the same layer.'}, {'input': 'Fortunately, the literature on network\npruning has observed that parameters of deep NNs have a lot of redundancy, and thus they can be\ncompressed without signi\x0ccantly a\x0becting performance', 'question': 'What are some methods used to prune parameters in neural networks?', 'ans': 'Some methods used to prune parameters in neural networks include pruning based on activations and pruning based on the similarity of weights to other neurons in the same layer.'}, {'input': 'Closer to our approach are methods that aim to prune neurons/channels that minimize the change\ninduced by pruning in the output of the layer being pruned, or its input to the next layer', 'question': 'What are some methods that aim to prune neurons/channels?', 'ans': 'Methods that aim to prune neurons/channels that minimize the change induced by pruning in the output of the layer being pruned, or its input to the next layer.'}, {'input': 'Although\nthese criteria are easier to optimize than the accuracy drop, they still yield an intractable combinato-\nrial problem. Zhuang et al', 'question': 'What is the intractable combinatorial problem that Zhuang et al. mention?', 'ans': 'The criteria that are easier to optimize than the accuracy drop yield an intractable combinatorial problem.'}, {'input': ', 2018; Ye et al., 2020).\n6 Empirical Evaluation\nIn this section, we examine the performance of our proposed pruning method in the limited-data\nregime. To that end, we focus on one-shot pruning, in which a pre-trained model has to be compressed\n8in a single step, without any \x0cne-tuning', 'question': 'What is the pruning method proposed by Zhuang et al. (2018)?', 'ans': 'Zhuang et al. (2018) propose to prune channels that minimize the change in the output, before the activation function, of the layer being pruned, and an additional discrimination-aware loss.'}, {'input': 'The pruning method in\n(Ye et al., 2020) is also based on minimizing the change in the output of the layer being pruned,\nbut after the activation function. They employ a greedy forward selection algorithm that provably\nsolves the`1-relaxation of the problem. Luo et al. (2017) and He et al', 'question': 'What is the pruning method in Ye et al., 2020 based on?', 'ans': 'The pruning method in Ye et al., 2020 is based on minimizing the change in the output of the layer being pruned, but after the activation function.'}, {'input': 'The pruning method in\n(Ye et al., 2020) is also based on minimizing the change in the output of the layer being pruned,\nbut after the activation function. They employ a greedy forward selection algorithm that provably\nsolves the`1-relaxation of the problem. Luo et al. (2017) and He et al', 'question': 'What is the pruning method used in Ye et al. (2020) based on?', 'ans': 'The pruning method in Ye et al. (2020) is based on minimizing the change in the output of the layer being pruned, but after the activation function.'}, {'input': 'Luo et al. (2017) and He et al. (2017) both propose to prune\nneurons/channels that minimize the change in the input to the next layer. Luo et al. (2017) solve\nthe resulting problem using a heuristic backward greedy algorithm, while He et al', 'question': 'What is the name of the algorithm that Luo et al. and He et al. propose to prune neurons/channels?', 'ans': 'They propose to prune neurons/channels that minimize the change in the input to the next layer.'}, {'input': 'Luo et al. (2017) and He et al. (2017) both propose to prune\nneurons/channels that minimize the change in the input to the next layer. Luo et al. (2017) solve\nthe resulting problem using a heuristic backward greedy algorithm, while He et al', 'question': 'What do Luo et al. (2017) and He et al. (2017) propose to do?', 'ans': 'They both propose to prune neurons/channels that minimize the change in the input to the next layer.'}, {'input': 'Luo et al. (2017) solve\nthe resulting problem using a heuristic backward greedy algorithm, while He et al. (2017) solve the\n`1-relaxation of the problem using alternating minimization. Most of these methods simultaneously\noptimize the weights of the layer being prune (Zhuang et al', 'question': 'What method did Luo et al. use to solve the problem?', 'ans': 'Luo et al. used a heuristic backward greedy algorithm to solve the problem.'}, {'input': 'We show that this selection problem\nis a weakly submodular maximization problem, thus it can be provably approximated using\nan e\x0ecient greedy algorithm. Our method is one of the few in the literature that uses only a\nlimited-number of training data and no labels', 'question': 'What is the approach used to solve the problem in the given text?', 'ans': 'The approach used to solve the problem in the given text is the 1-relaxation of the problem using alternating minimization.'}, {'input': ', 2017). Mariet and Sra (2015) depart from the\nusual strategy of pruning parameters whose removal in\ruences the network the least. They instead\nselect a subset of diverse neurons to keep in each layer, then optimize the weights of the next layer\nto minimize the change in the input to the next layer', 'question': "What is the difference between Mariet and Sra's strategy and the usual strategy of pruning parameters?", 'ans': "Mariet and Sra's strategy departs from the usual strategy of pruning parameters by not removing parameters whose removal influences the network the least."}, {'input': ', 2017). Mariet and Sra (2015) depart from the\nusual strategy of pruning parameters whose removal in\ruences the network the least. They instead\nselect a subset of diverse neurons to keep in each layer, then optimize the weights of the next layer\nto minimize the change in the input to the next layer', 'question': 'What is the approach taken by Mariet and Sra in their research?', 'ans': 'Mariet and Sra depart from the usual strategy of pruning parameters whose removal influences the network the least. They instead select a subset of diverse neurons to keep in each layer, then optimize the weights of the next layer to minimize the change in the input to the next layer.'}, {'input': 'The majority of existing structured pruning methods are heuristics that do not o\x0ber any theoretical\nguarantees. One exception is the work of (Ye et al., 2020), but their guarantee is w.r.t to the\n`1-relaxation of the problem', 'question': 'What is the majority of existing structured pruning methods?', 'ans': 'The majority of existing structured pruning methods are heuristics that do not offer any theoretical guarantees.'}, {'input': 'However, current structured pruning methods are highly empirical in nature,\ndo not provide any theoretical guarantees, and often require \x0cne-tuning, which makes them\ninapplicable in the limited-data regime', 'question': 'What is the main issue with existing structured pruning methods?', 'ans': 'The majority of existing structured pruning methods are heuristics that do not offer any theoretical guarantees.'}, {'input': 'One exception is the work of (Ye et al., 2020), but their guarantee is w.r.t to the\n`1-relaxation of the problem. Moreover, most pruning methods rely on \x0cne-tuning at least for a few\nepochs to boost the performance of the pruned NN', 'question': 'What is the exception mentioned in the text and what is the guarantee provided by the work of Ye et al.?', 'ans': 'The exception mentioned in the text is the work of Ye et al., and their guarantee is with respect to the 1-relaxation of the problem.'}, {'input': 'r.t to the\n`1-relaxation of the problem. Moreover, most pruning methods rely on \x0cne-tuning at least for a few\nepochs to boost the performance of the pruned NN. Fine-tuning is not possible in the limited-data\nregime, where only few training data is available, or data labels are unavailable', 'question': 'What is the limitation of fine-tuning in the limited-data regime?', 'ans': 'Fine-tuning is not possible in the limited-data regime, where only few training data is available, or data labels are unavailable.'}, {'input': 'Results in (Mariet\nand Sra, 2015) suggest that optimizing the weights to fuse information from the pruned neurons\ninto the remaining ones can provide a similar boost to performance as \x0cne-tuning, without the need\nfor data labels', 'question': 'What do the results in Mariet and Sra (2015) suggest about optimizing weights to fuse information from pruned neurons?', 'ans': 'The results suggest that optimizing the weights to fuse information from the pruned neurons into the remaining ones can provide a similar boost to performance as fine-tuning, without the need for data labels.'}, {'input': 'Our method is one of the few in the literature that uses only a\nlimited-number of training data and no labels. Our experimental results demonstrate that our\nmethod outperforms popular baseline methods in various one-shot pruning settings', 'question': 'What is the proposed method in the paper?', 'ans': "The proposed method in the paper is a structured pruning method that selects neurons to prune and new weights for the next layer, that minimize the change in the next layer's input induced by pruning."}, {'input': 'We refer to the replacement of the next layer\'s weights with the\nnew optimal weights as \\reweighting". The resulting subset selection problem is intractable, but we\nshow that it can be formulated as a weakly submodular maximization problem (see de\x0cnition in\nSection 2)', 'question': "What is the term used to refer to the replacement of the next layer's weights with the new optimal weights?", 'ans': "The term used is 'reweighting'."}, {'input': 'This formulation allows us to use the standard greedy selection algorithm to obtain\n(1\x00e\x00\r)-approximation to the optimal solution, where \ris non-zero if we use su\x0ecient training\ndata.\nOur method uses only limited training data and no labels', 'question': 'What is the approximation achieved by the standard greedy selection algorithm using the given formulation?', 'ans': 'The standard greedy selection algorithm achieves a (1e)-approximation to the optimal solution.'}, {'input': ', 2017). Mariet and Sra (2015) depart from the\nusual strategy of pruning parameters whose removal in\ruences the network the least. They instead\nselect a subset of diverse neurons to keep in each layer, then optimize the weights of the next layer\nto minimize the change in the input to the next layer', 'question': 'What is the observation made by Mariet and Sra in 2015?', 'ans': 'Mariet and Sra observed that reweighting provides a significant boost in performance not only to our method, but also to other baselines considered.'}, {'input': '2', 'question': 'What is the main observation made in this text?', 'ans': 'The main observation is that reweighting provides a significant boost in performance not only to the method being discussed, but also to other baselines considered.'}, {'input': 'Our method is one of the few in the literature that uses only a\nlimited-number of training data and no labels. Our experimental results demonstrate that our\nmethod outperforms popular baseline methods in various one-shot pruning settings', 'question': 'What is the focus of the method described in the text?', 'ans': 'The method focuses on pruning channels in CNNs and multiple layers in the network, by pruning each layer independently or sequentially.'}, {'input': 'Our method is one of the few in the literature that uses only a\nlimited-number of training data and no labels. Our experimental results demonstrate that our\nmethod outperforms popular baseline methods in various one-shot pruning settings', 'question': 'What do the experimental results demonstrate about the method?', 'ans': 'The experimental results demonstrate that the method often outperforms popular baselines and matches their performance even after fine-tuning.'}, {'input': 'Given anyi2S,I0=M(i), we have\nFm(I0jL0) =`m(xL0)\x00`m(xI0[L0)\n\x15`m(xL0)\x00`m(xL0\x00[r`m(xL0)]I0\n\x17jL0j+jI0j)\n\x15hr`m(xL0);[r`m(xL0)]I\n\x17jL0j+jI0ji\x00\x17jL0j+k0\n2k[r`m(xL0)]I\n\x17jL0j+jI0jk2\n2\n=k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j\nHence,\nG(SjL)\x14n`+1X\nm=1k[r`m(xL0)]S0k2\n2\n2\x16jL0j+k0=X\ni2S;I0=M(i)k[r`m(xL0)]I0k2\n2\n2\x16jL0j+k0\n=X\ni2S;I0=M(i)\x17jL0j+jI0j\n\x16jL0j+k0k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j=\x17jL0j+k0\n\x16jL0j+k0X\ni2SG(ijL):\nWe thus have \rU;k\x15\x16jU0j+k0\n\x17jU0j+jI0j', 'question': 'What does F(IjS) represent?', 'ans': 'The marginal gain of adding a set I to another set S, quantifying the change in value when adding I to S.'}, {'input': 'The cardinality of a set Sis written asjSj.\nGiven a vector x2Rd, we denote its support set by supp(x) =fi2Vjxi6= 0g, and its`2-norm\nbykxk2. Given a matrix X2Rd0\x02d, we denote its i-th column by Xi, and its Frobenius norm by\nkXkF', 'question': 'What is the support set of a vector x in R^d?', 'ans': 'The support set of a vector x in R^d is denoted by supp(x) = {i in V | xi != 0}.'}, {'input': 'Given a matrix X2Rd0\x02d, we denote its i-th column by Xi, and its Frobenius norm by\nkXkF. Given a set S\x12V,XSis the matrix with columns Xifor alli2S, and 0 otherwise, and 1S\nis the indicator vector of S, with [ 1S]i= 1 for alli2S, and 0 otherwise', 'question': 'What is the Frobenius norm of matrix X?', 'ans': 'kXkF'}, {'input': 'Weakly submodular maximization: A set function Fissubmodular if it has diminishing\nmarginal gains: F(ijS)\x15F(ijT) for allS\x12T,i2VnT. We say that Fisnormalized if\nF(;) = 0, and non-decreasing if F(S)\x14F(T) for allS\x12T', 'question': 'What is the definition of a submodular set function?', 'ans': 'A set function F is submodular if it has diminishing marginal gains: F(i∪S)−F(S) ≥ F(i∪T)−F(T) for all S⊆T and i∉T.'}, {'input': 'Weakly submodular maximization: A set function Fissubmodular if it has diminishing\nmarginal gains: F(ijS)\x15F(ijT) for allS\x12T,i2VnT. We say that Fisnormalized if\nF(;) = 0, and non-decreasing if F(S)\x14F(T) for allS\x12T', 'question': 'What is a set function called if it has diminishing marginal gains?', 'ans': 'A set function is called submodular if it has diminishing marginal gains.'}, {'input': 'Relaxed notions of submodularity/supermodularity, called weak DR-submodularity/supermodularity ,\nwere introduced in (Lehmann et al., 2006) and (Bian et al., 2017), respectively.\nDe\x0cnition B.1 (Weak DR-sub/supermodularity)', 'question': 'What is the definition of weak submodularity?', 'ans': "Weak submodularity is a notion introduced by Das and Kempe in 2011 which is sufficient to obtain provable bounds of the Greedy algorithm's performance."}, {'input': 'The parameter \rU;kis called the submodularity ratio ofF. It characterizes how close a set function\nis to being submodular. If Fis non-decreasing then \rU;k2[0;1], andFis submodular if and only\nif\rU;k= 1 for all U\x12V;k2N+', 'question': 'What is the submodularity ratio of F and what does it characterize?', 'ans': 'The submodularity ratio of F is U;k and it characterizes how close a set function is to being submodular.'}, {'input': "5.1 Reweighted input change pruning variants\nWe consider three variants of our method: LayerInChange ,SeqInChange , and AsymInChange .\nInLayerInChange , we prune each layer independently, i.e., we apply exactly the method in\n7Section 3 or 4, according to the layer's type", 'question': 'What is the purpose of reweighted input change pruning?', 'ans': 'The purpose of reweighted input change pruning is to prune neurons in a single layer.'}, {'input': ', 2019). In this work, we follow the network pruning approach, where the redundant\nunits (weights, neurons or \x0clters/channels) of a pre-trained NN are removed; see (Kuzmin et al.,\n∗part of this work was done while a postdoc at MIT, CSAIL.\n1arXiv:2203.04940v1  [cs.LG]  9 Mar 20222019; Blalock et al', 'question': 'What is the goal of pruning a pre-trained neural network?', 'ans': 'The goal is to select a small number of neurons to keep and prune the rest in a way that influences the network the least.'}, {'input': 'One way to achieve this is by minimizing the change in input to the next\nlayer,`+ 1, induced by pruning. However, simply throwing away the activations from the dropped\nneurons is wasteful. Instead, we optimize the weights of the next layer to reconstruct the inputs\nfrom the remaining neurons', 'question': 'What is the purpose of optimizing the weights of the next layer in neural network pruning?', 'ans': 'To reconstruct the inputs from the remaining neurons and minimize the change in input to the next layer.'}, {'input': '(3). Though, this requires a somewhat stronger assumption: any k+ 1 columns\nofA`should be linearly independent and all rows of W`+1should be linearly independent. In\nparticular, we would need that n`\x14n`+1, which is not always satis\x0ced.\n3', 'question': 'What are A and W+ in the given text?', 'ans': 'A is the activation matrix of layer `with columns a` and W+ is the weight matrix of layer `+ 1 with columns w`+1'}, {'input': 'When a neuron is pruned in layer `, the corresponding column of weights in W`and row in W`+1\nare removed. Pruning n`\x00kneurons in layer `reduces the number of parameters and computation\ncost by (n`\x00k)=n`for both layer `and`+ 1.\nLetV`=f1;\x01\x01\x01;n`g', 'question': 'What happens to the corresponding column of weights in W` and row in W`+1 when a neuron is pruned in layer `?', 'ans': 'They are removed.'}, {'input': 'Closer to our approach are methods that aim to prune neurons/channels that minimize the change\ninduced by pruning in the output of the layer being pruned, or its input to the next layer', 'question': 'What is the purpose of pruning neurons in a layer?', 'ans': 'Pruning neurons in a layer reduces the number of parameters and computation cost.'}, {'input': 'e., replacing the weights W`+1of layer`+ 1 with the new weights ~W`+1.\n43.1 Greedy selection\nSolving Problem (1)exactly is NP-Hard (Natarajan, 1995). However, we show below that it can be\nformulated as a weakly submodular maximization problem, hence it can be e\x0eciently approximated', 'question': 'Is solving Problem (1) exactly possible?', 'ans': 'No, it is NP-Hard.'}, {'input': 'pdf .\n16A Missing proofs\nRecall that F(S) =kA`W`+1k2\nF\x00min ~W`+12Rn`\x02n`+1kA`W`+1\x00A`\nS~W`+1k2\nF, andG(S) =F(M(S)),\nwhereMmaps each channel to its corresponding columns in A`. We denote by ~F(S) the objective\ncorresponding to the asymmetric formulation introduced in Section 5.1, i.e', 'question': 'What is the purpose of F(S) in the given text?', 'ans': 'F(S) is used to represent the sum of n+1 sparse linear regression problems.'}, {'input': 'They instead\nselect a subset of diverse neurons to keep in each layer, then optimize the weights of the next layer\nto minimize the change in the input to the next layer.\nThe majority of existing structured pruning methods are heuristics that do not o\x0ber any theoretical\nguarantees', 'question': 'What algorithm is used to select a set of neurons to keep in a layer?', 'ans': 'The Greedy algorithm is used to select a set of neurons to keep in a layer.'}, {'input': '1).\nWe use the Greedy algorithm to select a set ^S\x12V`ofkneurons to keep in layer `. As discussed\nin Section 2, the returned solution is guaranteed to satisfy\nF(^S)\x15(1\x00e\x00\r^S;k) max\njSj\x14kF(S) (3)\nThe submodularity ratio \r^S;kis non-zero if any 2 kcolumns of A`are linearly independent', 'question': 'What is the purpose of using the Greedy algorithm in selecting a set of neurons to keep in layer `?', 'ans': 'The purpose of using the Greedy algorithm is to select a set of neurons to keep in layer `.'}, {'input': 'If the\nnumber of training data is larger than the number of neurons, i.e., n > n`, this is likely to be\nsatis\x0ced for any k\x14n`=2; it is unlikely for the activations of a neuron to match exactly a linear\ncombination of other neurons. We verify that this is indeed the case in our experiments', 'question': 'What is the condition for n > n`?', 'ans': 'If the number of training data is larger than the number of neurons.'}, {'input': 'We verify that this is indeed the case in our experiments.\nWe show in Appendix B that Fsatis\x0ces an even stronger notion of approximate submodularity\nthan weak submodularity, which implies a better approximation guarantee for Greedy than the\none provided in Eq. (3)', 'question': 'What does Appendix B show about Fsatis?', 'ans': 'Appendix B shows that Fsatis satisfies an even stronger notion of approximate submodularity than weak submodularity, which implies a better approximation guarantee for Greedy than the one provided in Eq. (3).'}, {'input': '(3). Though, this requires a somewhat stronger assumption: any k+ 1 columns\nofA`should be linearly independent and all rows of W`+1should be linearly independent. In\nparticular, we would need that n`\x14n`+1, which is not always satis\x0ced.\n3', 'question': 'What assumption is required for k+1 columns of A to be linearly independent?', 'ans': 'Any k+1 columns of A should be linearly independent and all rows of W+1 should be linearly independent.'}, {'input': 'Similar to (Mariet and Sra, 2015), we\nobserve that reweighting provide a signi\x0ccant boost in performance not only to our method, but also\nto other baselines we consider. However unlike (Mariet and Sra, 2015), we only use a small fraction\nof the training data, around \x181% in our experiments', 'question': 'What is the purpose of reweighting in the given text?', 'ans': 'The purpose of reweighting is to minimize the reweighted input change.'}, {'input': 'Similar to (Mariet and Sra, 2015), we\nobserve that reweighting provide a signi\x0ccant boost in performance not only to our method, but also\nto other baselines we consider. However unlike (Mariet and Sra, 2015), we only use a small fraction\nof the training data, around \x181% in our experiments', 'question': 'What is the reweighting procedure introduced in Mariet and Sra, 2015?', 'ans': 'The reweighting procedure introduced in Mariet and Sra, 2015 is the same as the new weights merge the weights from the dropped neurons into the selected ones.'}, {'input': '5This is the same reweighting procedure introduced in (Mariet and Sra, 2015). But instead of applying\nit only at the end to the selected neurons ^S, it is implicitly done at each iteration of our pruning\nmethod, as it is required to evaluate F. We discuss next how this can be done e\x0eciently.\n3', 'question': 'What is the reweighting procedure discussed in the text?', 'ans': 'The reweighting procedure discussed in the text is the same one introduced in (Mariet and Sra, 2015), but it is implicitly done at each iteration of the pruning method instead of only at the end to the selected neurons ^S, as it is required to evaluate F.'}, {'input': 'We discuss next how this can be done e\x0eciently.\n3.3 Cost\nEach iteration of Greedy requiresO(n`) function evaluations of F. Computing F(S) from scratch\nneedsO(k\x01(n`\x01n`+1+n\x01(n`+n`+1)) time, so a naive implementation of Greedy is too expensive', 'question': 'What is the time complexity of a naive implementation of Greedy algorithm?', 'ans': 'O(k(n^2+n+1))'}, {'input': 'Proposition 3.2. GivenS\x12V`such thatjSj\x14k,i62S, let projS(a`\nj) =A`\nSxS(a`\nj)be the\nprojection of a`\njonto the column space of A`\nS,RS(a`\ni) =a`\ni\x00projS(a`\ni)and projRS(a`\ni)(a`\nj)2\narg minz=RS(a`\ni)\r;\r2Rka`\nj\x00zk2\n2the corresponding residual and the projection of a`\njonto it', 'question': 'What is Proposition 3.2 about?', 'ans': 'Proposition 3.2 outlines how to efficiently evaluate F(S+i) given that F(S) was computed in the previous iteration.'}, {'input': 'Hence, projS[i(y) =projS(y) +projRS(di)(y), where\nprojRS(di)(y) =RS(di)\rS;i(y)', 'question': 'What is the definition of projS(a`j)?', 'ans': 'projS(a`j) is the projection of a`j onto the column space of A`S.'}, {'input': 'Given anyi2S,I0=M(i), we have\nFm(I0jL0) =`m(xL0)\x00`m(xI0[L0)\n\x15`m(xL0)\x00`m(xL0\x00[r`m(xL0)]I0\n\x17jL0j+jI0j)\n\x15hr`m(xL0);[r`m(xL0)]I\n\x17jL0j+jI0ji\x00\x17jL0j+k0\n2k[r`m(xL0)]I\n\x17jL0j+jI0jk2\n2\n=k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j\nHence,\nG(SjL)\x14n`+1X\nm=1k[r`m(xL0)]S0k2\n2\n2\x16jL0j+k0=X\ni2S;I0=M(i)k[r`m(xL0)]I0k2\n2\n2\x16jL0j+k0\n=X\ni2S;I0=M(i)\x17jL0j+jI0j\n\x16jL0j+k0k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j=\x17jL0j+k0\n\x16jL0j+k0X\ni2SG(ijL):\nWe thus have \rU;k\x15\x16jU0j+k0\n\x17jU0j+jI0j', 'question': 'What is the formula for F(ijS)?', 'ans': 'F(ijS) =n`+1X m=1kprojRS(a`i)(A`VnS)w`+1mk22;'}, {'input': "In total Greedy 's runtime is then O(k\x01(n`)2\x01(n`+1+n+k)). Using a faster variant of Greedy\ncalled Stochastic-Greedy (Mirzasoleiman et al., 2015) further reduces the cost to O(log(1=\x0f)\x01\n(n`)2\x01(n`+1+n+k)) while maintaining almost the same approximation guarantee (1 \x00e\x00\r^S;k\x00\x0f)\nin expectation", 'question': 'What is the runtime of Greedy algorithm?', 'ans': "The runtime of Greedy algorithm is O(k(n')^2(n'+1+n+k))."}, {'input': 'As discussed in Section B.1, Proposition B.3 implies that Greedy achieves a (1\x00e\x00\x0b2k\x001)-\napproximation with F(S), where\x0bkis non-zero if all rows of A`W`+1are linearly independent', 'question': 'What is the name of the faster variant of Greedy mentioned in the text?', 'ans': 'Stochastic-Greedy'}, {'input': '4 Pruning regular regions of neurons\nIn this section, we discuss how to adapt our approach to pruning regular regions of neurons. This\nis easily achieved by mapping any set of regular regions to the corresponding set of neurons, then\napplying the same method in Section 3', 'question': 'What is discussed in the section about pruning regular regions of neurons?', 'ans': 'Adapting the approach to pruning regular regions of neurons'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is discussed in section 4 of the text?', 'ans': 'The adaptation of the approach to pruning regular regions of neurons.'}, {'input': '4 Pruning regular regions of neurons\nIn this section, we discuss how to adapt our approach to pruning regular regions of neurons. This\nis easily achieved by mapping any set of regular regions to the corresponding set of neurons, then\napplying the same method in Section 3', 'question': 'What is the focus of the method described in Section 3?', 'ans': 'Pruning channels in CNNs'}, {'input': 'e.,A`= [X`\n1;\x01\x01\x01;X`\nn`]. Similarly, we arrange the weights F`+1\nc2Rn`+1\x02rh\x02rw\nof each channel cintorh\x01rwrows ofW`+12Rn`\x01rh\x01rw\x02n`+1, i.e., (W`+1)>= [(F`\n1)>;\x01\x01\x01;(F`\nn`)>].\nRecall that V`=f1;\x01\x01\x01;n`g, and letV0\n`=f1;\x01\x01\x01;rhrwn`g', 'question': 'What are X and F in the given text?', 'ans': 'X represents the activations for each output channel and training input, while F represents the weights of layer +1, corresponding to n filters of size rh x rw for each of its output channels.'}, {'input': '1The results in (Mirzasoleiman et al., 2015) only apply to submodular functions, but it is straighforward to extend\nthem to weakly submodular functions.\n6When an output channel is pruned in layer `, the corresponding weights in F`andF`+1are removed', 'question': 'What functions do the results in Mirzasoleiman et al. (2015) apply to?', 'ans': 'The results in Mirzasoleiman et al. (2015) only apply to submodular functions.'}, {'input': '6When an output channel is pruned in layer `, the corresponding weights in F`andF`+1are removed.\nPruningn`\x00koutput channels in layer `reduces the number of parameters and computation cost\nby (n`\x00k)=n`for both layer `and`+ 1', 'question': 'What happens to the corresponding weights in F` and F`+1 when an output channel is pruned in layer `?', 'ans': 'The corresponding weights in F` and F`+1 are removed.'}, {'input': 'Pruningn`\x00koutput channels in layer `reduces the number of parameters and computation cost\nby (n`\x00k)=n`for both layer `and`+ 1. If layer `is followed by a batch norm layer, the weights\ntherein corresponding to the pruned channels are also removed', 'question': 'What is the benefit of pruning output channels in a layer?', 'ans': 'Pruning output channels in a layer reduces the number of parameters and computation cost by (n`k)=n` for both layer and +1.'}, {'input': 'If layer `is followed by a batch norm layer, the weights\ntherein corresponding to the pruned channels are also removed.\nWe arrange the activations X`\nc2Rn\x01p`\x02rh\x01rwof each channel cintorhrwcolumns of A`2\nRn\x01p`\x02n`\x01rh\x01rw, i.e.,A`= [X`\n1;\x01\x01\x01;X`\nn`]', 'question': 'What happens to the weights of a layer when it is followed by a batch norm layer?', 'ans': 'The weights corresponding to the pruned channels are also removed.'}, {'input': 'e.,A`= [X`\n1;\x01\x01\x01;X`\nn`]. Similarly, we arrange the weights F`+1\nc2Rn`+1\x02rh\x02rw\nof each channel cintorh\x01rwrows ofW`+12Rn`\x01rh\x01rw\x02n`+1, i.e., (W`+1)>= [(F`\n1)>;\x01\x01\x01;(F`\nn`)>].\nRecall that V`=f1;\x01\x01\x01;n`g, and letV0\n`=f1;\x01\x01\x01;rhrwn`g', 'question': 'What is the purpose of arranging the weights of each channel in the given text?', 'ans': 'To form a matrix (W+1) of weights for each channel'}, {'input': 'We de\x0cne a function M: 2V`!2V0\n`\nwhich maps every channel cto its corresponding rhrwcolumnsA`. LetG(S) =F(M(S)), withF\nde\x0cned in Eq. (2), then minimizing the reweighted input change kA`W`+1\x00A`\nM(S)~W`+1k2\nFwith\na budgetkis equivalent to maxjSj\x14kG(S)', 'question': 'What is the function M defined as?', 'ans': 'The function M is defined as a mapping from every channel c to its corresponding rh x rw columns A.'}, {'input': '17LetS0=M(S);L0=M(L),I0=M(i), andk0=rhrwk. As before, we can write G(S) =F(S0) =Pn`+1\nm=1Fm(S0) :=`m(0)\x00min supp( ~wm)\x12S0`m(~wm), where`m(~wm) =kA`w`+1\nm\x00A`~wmk2\n2. We denote\nby\x16kand\x17kthe RSC and RSM parameters of `m, given by\x16k=minkzk2=1;kzk0\x14kkA`zk2\n2, and\n\x17k= maxkzk2=1;kzk0\x14kkA`zk2\n2', 'question': 'What is the function M in the given text?', 'ans': "The function M is defined as a mapping from every channel c to its corresponding rhrw columns A'."}, {'input': 'We de\x0cne a function M: 2V`!2V0\n`\nwhich maps every channel cto its corresponding rhrwcolumnsA`. LetG(S) =F(M(S)), withF\nde\x0cned in Eq. (2), then minimizing the reweighted input change kA`W`+1\x00A`\nM(S)~W`+1k2\nFwith\na budgetkis equivalent to maxjSj\x14kG(S)', 'question': 'What is the function M defined as?', 'ans': 'The function M maps every channel c to its corresponding rhrwcolumnsA`.'}, {'input': 'LetG(S) =F(M(S)), withF\nde\x0cned in Eq. (2), then minimizing the reweighted input change kA`W`+1\x00A`\nM(S)~W`+1k2\nFwith\na budgetkis equivalent to maxjSj\x14kG(S). The following proposition shows that this remains a\nweakly submodular maximization problem.\nProposition 4.1', 'question': 'What is the proposition number that shows that minimizing a reweighted input change with a budget k is equivalent to a weakly submodular maximization problem?', 'ans': 'Proposition 4.1'}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What does Proposition 4.1 state about the given function U?', 'ans': 'Proposition 4.1 states that given U, a normalized non-decreasing U;k-weakly submodular function, the problem remains a weakly submodular maximization problem.'}, {'input': 'We extend the relation established in\n(Elenberg et al., 2016; Das and Kempe, 2011) between weak submodularity and sparse eigenvalues\nof the covariance matrix to this case. The proof then follows in a similar way to Proposition 3.1', 'question': 'What is the relation established in Elenberg et al?', 'ans': 'The text does not provide enough information to answer this question.'}, {'input': 'We extend the relation established in\n(Elenberg et al., 2016; Das and Kempe, 2011) between weak submodularity and sparse eigenvalues\nof the covariance matrix to this case. The proof then follows in a similar way to Proposition 3.1', 'question': 'What is the relation extended in Elenberg et al. and Das and Kempe?', 'ans': 'The relation between weak submodularity and sparse eigenvalues of the covariance matrix.'}, {'input': '1.\nAs before, we use the Greedy algorithm, with function G, to select a set ^S\x12V`ofkchannels to\nkeep in layer `. We get the same approximation guarantee G(^S)\x15(1\x00e\x00\r^S;k)maxjSj\x14kG(S):The\nsubmodularity ratio \r^S;kis non-zero if any 2 krhrwcolumns of A`are linearly independent', 'question': 'What algorithm is used to select a set of channels to keep in layer `?', 'ans': 'The Greedy algorithm, with function G, is used to select a set of channels to keep in layer `.'}, {'input': 'We get the same approximation guarantee G(^S)\x15(1\x00e\x00\r^S;k)maxjSj\x14kG(S):The\nsubmodularity ratio \r^S;kis non-zero if any 2 krhrwcolumns of A`are linearly independent. In our\nexperiments, we observe that linear independence only holds for small k\x140:2n`in certain layers', 'question': 'What is the condition for the submodularity ratio ^S;k to be non-zero?', 'ans': 'The submodularity ratio ^S;k is non-zero if any 2 krhrwcolumns of A`are linearly independent.'}, {'input': 'This is due to the correlation between patches which overlap. To remedy that, we experimented\nwith using only rhrwrandom patches from each image, instead of using all patches. This indeed\nraises the rank of A`, such that the range where linear independence is satis\x0ced is around k\x140:4n`', 'question': 'What did the experimenters do to remedy the correlation between overlapping patches?', 'ans': 'They used only rhrwrandom patches from each image instead of using all patches.'}, {'input': 'This indeed\nraises the rank of A`, such that the range where linear independence is satis\x0ced is around k\x140:4n`.\nHowever, the obtained results are very similar to the ones with all patches, we thus omit them', 'question': 'What is the effect of raising the rank of A`?', 'ans': 'The range where linear independence is satisfied is around k=0.4n`.'}, {'input': '(3). Though, this requires a somewhat stronger assumption: any k+ 1 columns\nofA`should be linearly independent and all rows of W`+1should be linearly independent. In\nparticular, we would need that n`\x14n`+1, which is not always satis\x0ced.\n3', 'question': 'What does having linear dependence not necessarily imply?', 'ans': 'Having linear dependence does not necessarily imply that the method does not perform well.'}, {'input': 'Note\nthat having linear dependence does not necessarily imply that \r^S;k= 0; our method still performs\nwell in these cases.\nFor a \x0cxed S\x12V`, the optimal weights are again given by ~W`+1=xM(S)(A`)W`+1. The cost of\nrunning Greedy and reweighting is the same as before (see Appendix A.2)', 'question': 'Does linear dependence necessarily imply that the method performs poorly?', 'ans': 'No, linear dependence does not necessarily imply that the method performs poorly.'}, {'input': '4 Pruning regular regions of neurons\nIn this section, we discuss how to adapt our approach to pruning regular regions of neurons. This\nis easily achieved by mapping any set of regular regions to the corresponding set of neurons, then\napplying the same method in Section 3', 'question': 'What is the method explained in this section?', 'ans': 'The method explained in this section is how to apply pruning to multiple layers of a NN.'}, {'input': '2).\n•In all one shot pruning settings we considered, reweighting signi\x0ccantly improves performance\nfor all methods, and reduces variance in results.\n•The choice of how much to prune in each layer given a global budget can have a drastic e\x0bect\non performance, as illustrated in Appendix F', 'question': 'What are the three variants of the pruning method explained in section 5.1?', 'ans': 'The three variants of the pruning method explained in section 5.1 are LayerInChange, SeqInChange, and AsymInChange.'}, {'input': "5.1 Reweighted input change pruning variants\nWe consider three variants of our method: LayerInChange ,SeqInChange , and AsymInChange .\nInLayerInChange , we prune each layer independently, i.e., we apply exactly the method in\n7Section 3 or 4, according to the layer's type", 'question': 'What are the three variants of the reweighted input change pruning method?', 'ans': 'The three variants are LayerInChange, SeqInChange, and AsymInChange.'}, {'input': 'For fair comparison and to disentangle the bene\x0ct of using our pruning\nmethod from the bene\x0ct of reweighting (Section 3.2), we report results with reweighting applied to\nall pruning methods, or none of them', 'question': 'What is the advantage of applying the method in Section 3 or 4?', 'ans': 'It is the fastest variant and has the same cost as pruning a single layer, as each layer can be pruned in parallel, and it only requires one forward pass to get the activations of all layers.'}, {'input': 'This is the fastest variant; it has the same cost as\npruning a single layer, as each layer can be pruned in parallel, and it only requires one forward pass\nto get the activations of all layers. However, it does not take into account the e\x0bect of pruning one\nlayer on subsequent layers', 'question': 'What is the advantage of the fastest variant of pruning?', 'ans': 'It has the same cost as pruning a single layer, as each layer can be pruned in parallel, and it only requires one forward pass to get the activations of all layers.'}, {'input': 'We further adapt our method to prune (1)\nany regular regions of weights; we focus in particular on pruning channels in convolution neural\nnetworks (CNNs), and (2) multiple layers in the network, by pruning each layer independently or\nsequentially', 'question': 'What is the concern with pruning one layer in a neural network?', 'ans': 'It does not take into account the effect of pruning one layer on subsequent layers.'}, {'input': 'However, it does not take into account the e\x0bect of pruning one\nlayer on subsequent layers.\nInSeqInChange , we prune each layer sequentially, starting from the earliest layer to latest one', 'question': 'What is the pruning method used in InSeqInChange?', 'ans': 'InSeqInChange prunes each layer sequentially, starting from the earliest layer to the latest one.'}, {'input': '•Among the three variants of our method, AsymInChange usually performs the best, and\nLayerInChange the worst. But in some settings, LayerInChange yields the best results\n(see Fig. 2)', 'question': 'What is the asymmetric formulation used in InAsymInChange?', 'ans': 'The asymmetric formulation used in InAsymInChange is to approximate the original inputA`W`+1 instead of approximating the updated inputB`W`+1 to avoid the accumulation of error.'}, {'input': 'e., we solve\nminjSj\x14k;~W`+12Rn`\x02n`+1kA`W`+1\x00B`\nS~W`+1k2\nF. This problem is still a weakly submodular max-\nimization problem, with the same submodularity ratio given in Propositions 3.1 and 4.1, with\nA`replaced by B`therein (see Appendix A.1)', 'question': 'What is the problem that is being solved in this text?', 'ans': 'The problem being solved is a weakly submodular maximization problem.'}, {'input': '2', 'question': 'What is the problem described in the text?', 'ans': 'The problem is a weakly submodular maximization problem.'}, {'input': '1, with\nA`replaced by B`therein (see Appendix A.1). Hence, the same approximation guarantee as in\nthe symmetric formulation holds here. Moreover, a better approximation guarantee can again be\nobtained with a stronger notion of approximate submodularity, under stronger assumptions (see\nAppendix B)', 'question': 'What is the stronger notion of approximate submodularity?', 'ans': 'The stronger notion of approximate submodularity is mentioned in Appendix B.'}, {'input': 'Moreover, a better approximation guarantee can again be\nobtained with a stronger notion of approximate submodularity, under stronger assumptions (see\nAppendix B). The cost of running Greedy with the asymmetric formulation and reweighting is\nalso the same as before (see Appendix A.2)', 'question': 'What can be done to obtain a better approximation guarantee?', 'ans': 'A better approximation guarantee can be obtained with a stronger notion of approximate submodularity, under stronger assumptions.'}, {'input': '2).\nWe evaluate all three variants in our experiments. As expected, AsymInChange usually performs\nthe best, and LayerInChange the worst. But in some settings, LayerInChange yields the best\nresults (see Fig. 2).\n5', 'question': 'Which variant usually performs the best in the experiments?', 'ans': 'AsymInChange'}, {'input': '2).\nWe evaluate all three variants in our experiments. As expected, AsymInChange usually performs\nthe best, and LayerInChange the worst. But in some settings, LayerInChange yields the best\nresults (see Fig. 2).\n5', 'question': 'Which variant usually performs the best in the experiments?', 'ans': 'AsymInChange'}, {'input': '2 Per-layer budget selection\nAnother important design choice is how much to prune in each layer, given a desired global compres-\nsion ratio (see Appendix F for the e\x0bect of this choice on performance). In our experiments, we use\nthe compression ratio selection method introduced in (Kuzmin et al', 'question': 'What is an important design choice when it comes to pruning?', 'ans': 'Another important design choice is how much to prune in each layer, given a desired global compression ratio.'}, {'input': 'F Importance of per-layer budget selection\nIn this section, we study the e\x0bect of per-layer budget selection on accuracy. To that end, we\nconsider the same setup as in Section 6.2, but instead of pruning the \x0crst two convolutional layers\nof VGG11, we prune the \x0crst and second to last layer', 'question': 'What is the importance of per-layer budget selection in network pruning?', 'ans': 'Per-layer budget selection determines how much to prune in each layer, given a desired global compression ratio.'}, {'input': 'In our experiments, we use\nthe compression ratio selection method introduced in (Kuzmin et al., 2019, Section 3.4.1), which\ncan be applied to any layerwise pruning method, thus enabling us to have a fair comparison', 'question': 'What method did the experiments use for compression ratio selection?', 'ans': 'The experiments used the compression ratio selection method introduced in (Kuzmin et al., 2019, Section 3.4.1).'}, {'input': ', 2019, Section 3.4.1), which\ncan be applied to any layerwise pruning method, thus enabling us to have a fair comparison.\nGiven a network with Llayers to prune, let k=\x0bPL\n`=1n`, for some\x0b2[0;1], be the global number\nof neurons/channels to keep', 'question': 'What is the value of k in the pruning method described in the text?', 'ans': 'k=PL^(1-n), for some 2[0;1], where L is the number of layers to prune and P is the global number of neurons/channels to keep.'}, {'input': ', 2019, Section 3.4.1), which\ncan be applied to any layerwise pruning method, thus enabling us to have a fair comparison.\nGiven a network with Llayers to prune, let k=\x0bPL\n`=1n`, for some\x0b2[0;1], be the global number\nof neurons/channels to keep', 'question': 'What is the value of k in the given network with L layers to prune?', 'ans': 'k=PL`=1n`, for some 2[0;1], be the global number of neurons/channels to keep.'}, {'input': 'g., \x0b`2f0:05;0:1;\x01\x01\x01;1g. We\nde\x0cne a layerwise accuracy metric P`(k`) as the accuracy obtained after pruning layer `, with a\nbudgetk`, while other layers are kept intact, evaluated on a veri\x0ccation set. We set aside a subset\nof the training set to use as a veri\x0ccation set', 'question': 'What is the layerwise accuracy metric P`(k`)?', 'ans': 'The layerwise accuracy metric P`(k`) is the accuracy obtained after pruning layer `, with a budgetk`, while other layers are kept intact, evaluated on a verification set.'}, {'input': 'We set aside a subset\nof the training set to use as a veri\x0ccation set. Let Porigbe the original model accuracy', 'question': 'What is the purpose of the verification set in the training process?', 'ans': 'The verification set is used to verify the accuracy of the model.'}, {'input': '2 Per-layer budget selection\nAnother important design choice is how much to prune in each layer, given a desired global compres-\nsion ratio (see Appendix F for the e\x0bect of this choice on performance). In our experiments, we use\nthe compression ratio selection method introduced in (Kuzmin et al', 'question': 'What is the alternative strategy mentioned for achieving the desired global compression ratio?', 'ans': 'Pruning each layer until the per-layer error reaches some threshold and varying the threshold to obtain the desired global compression ratio.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the focus of the empirical evaluation in this section?', 'ans': 'The focus is on the performance of the proposed pruning method in the limited-data regime, specifically on one-shot pruning.'}, {'input': 'To that end, we focus on one-shot pruning, in which a pre-trained model has to be compressed\n8in a single step, without any \x0cne-tuning. We study the e\x0bect of \x0cne-tuning in Appendix E', 'question': 'What is one-shot pruning?', 'ans': 'One-shot pruning is a compression technique in which a pre-trained model is compressed in a single step, without any fine-tuning.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the purpose of the study mentioned in the text?', 'ans': 'The purpose of the study is to examine the effect of fine-tuning.'}, {'input': 'When both \x0cne-tuning and reweighting are used, all methods\nperform fairly similarly, except WeightNorm which consistently performs worst, and random\npruning methods which sometimes perform worst (see Figures 13 and 14)', 'question': 'What is the WeightNorm pruning technique?', 'ans': 'WeightNorm is a pruning technique that prunes neurons/channels with the lowest output weights 1-norm, normalized by the number of neurons/channels in their layer.'}, {'input': 'This is the \\onorm" function proposed in\n(He et al., 2014) for neuron pruning, which provides the best results among the three neuron\'s\nimportance criteria considered therein.\n•LayerWeightNorm : prunes neurons/channels with the lowest output weights `1-norm, in\neach layer', 'question': "What is the 'onorm' function proposed in (He et al., 2014) used for?", 'ans': "The 'onorm' function proposed in (He et al., 2014) is used for neuron pruning."}, {'input': '•LayerWeightNorm : prunes neurons/channels with the lowest output weights `1-norm, in\neach layer. This is the layerwise version of WeightNorm . It was proposed in (Li et al., 2017)\nfor channel pruning', 'question': 'What is LayerWeightNorm and what is it used for?', 'ans': 'LayerWeightNorm is a method that prunes neurons/channels with the lowest output weights in each layer. It is used for channel pruning.'}, {'input': '(2017) solve the\n`1-relaxation of the problem using alternating minimization. Most of these methods simultaneously\noptimize the weights of the layer being prune (Zhuang et al., 2018) or the next one (He et al., 2017),\nor do so in a separate step at the end (Luo et al., 2017)', 'question': 'What is the purpose of the layerwise version of WeightNorm proposed in Li et al. (2017)?', 'ans': 'The purpose of the layerwise version of WeightNorm proposed in Li et al. (2017) is for channel pruning.'}, {'input': ', 2017).\n•LayerActGrad : prunes neurons/channels with the lowest (activations \x02gradients), averaged\nover the training data, in each layer. This is the layerwise variant of ActGrad .\nWe evaluate the performance of these methods on neuron pruning of the LeNet model (LeCun et al', 'question': 'What is ActGrad and how does it prune neurons/channels?', 'ans': 'ActGrad is a criteria used for pruning neurons/channels. It prunes the neurons/channels with the lowest activations gradients, averaged over the training data, with layerwise 2-normalization.'}, {'input': 'This criteria is derived in (Molchanov et al.,\n2017) from the \x0crst order Taylor approximation of the change in loss induced by pruning. It\nis the best performing criteria among the ones evaluated in (Molchanov et al., 2017)', 'question': 'What is the criteria derived from in Molchanov et al., 2017?', 'ans': 'The criteria is derived from the first order Taylor approximation of the change in loss induced by pruning.'}, {'input': ', 2017).\n•LayerActGrad : prunes neurons/channels with the lowest (activations \x02gradients), averaged\nover the training data, in each layer. This is the layerwise variant of ActGrad .\nWe evaluate the performance of these methods on neuron pruning of the LeNet model (LeCun et al', 'question': 'What is LayerActGrad?', 'ans': 'LayerActGrad is a criteria that prunes neurons/channels with the lowest (activations gradients), averaged over the training data, in each layer. It is the layerwise variant of ActGrad and is considered the best performing criteria among the ones evaluated in (Molchanov et al., 2017).'}, {'input': ', 2017).\n•LayerActGrad : prunes neurons/channels with the lowest (activations \x02gradients), averaged\nover the training data, in each layer. This is the layerwise variant of ActGrad .\nWe evaluate the performance of these methods on neuron pruning of the LeNet model (LeCun et al', 'question': 'What is the LayerActGrad method used for?', 'ans': 'LayerActGrad method is used for pruning neurons/channels with the lowest (activations gradients), averaged over the training data, in each layer.'}, {'input': 'We evaluate the performance of these methods on neuron pruning of the LeNet model (LeCun et al.,\n1989) on the MNIST dataset (Lecun et al., 1998), and on both neuron and channel pruning of the\nVGG11 model (Simonyan and Zisserman, 2015) on the CIFAR-10 dataset (Krizhevsky et al., 2009)', 'question': 'What is the name of the method being evaluated on neuron pruning of the LeNet model?', 'ans': 'The method being evaluated is the layerwise variant of ActGrad.'}, {'input': 'We evaluate the performance of these methods on neuron pruning of the LeNet model (LeCun et al.,\n1989) on the MNIST dataset (Lecun et al., 1998), and on both neuron and channel pruning of the\nVGG11 model (Simonyan and Zisserman, 2015) on the CIFAR-10 dataset (Krizhevsky et al., 2009)', 'question': 'What models and datasets were used to evaluate the performance of the methods?', 'ans': 'The LeNet model on the MNIST dataset and the VGG11 model on the CIFAR-10 dataset were used to evaluate the performance of the methods.'}, {'input': '6.3 Transfer learning\nIn this section, we consider a transfer learning setting, where given a large pre-trained network,\nour goal is to obtain a small model on a di\x0berent but related task to the one it was trained on', 'question': 'What is the transfer learning setting considered in the text?', 'ans': 'The transfer learning setting considered in the text involves pretraining VGG11 on CIFAR-10, then fine-tuning, pruning and evaluating it on MNIST.'}, {'input': '6.3 Transfer learning\nIn this section, we consider a transfer learning setting, where given a large pre-trained network,\nour goal is to obtain a small model on a di\x0berent but related task to the one it was trained on', 'question': 'What is the transfer learning setting considered in the text?', 'ans': 'The transfer learning setting considered in the text involves pretraining VGG11 on CIFAR-10, then fine-tuning, pruning and evaluating it on MNIST.'}, {'input': 'We implemented all the compared pruning methods using Pytorch (Paszke et al., 2017). Our code\nbuilds on the open source ShrinkBench library introduced in (Blalock et al., 2020), and uses the\ncode from (Buschj\x7f ager et al., 2020) for Greedy', 'question': 'What library did the authors use to implement the compared pruning methods?', 'ans': 'The authors used Pytorch to implement the compared pruning methods.'}, {'input': ', 2017). Our code\nbuilds on the open source ShrinkBench library introduced in (Blalock et al., 2020), and uses the\ncode from (Buschj\x7f ager et al., 2020) for Greedy . We use the implementation of LeNet included in\nShrinkBench (Blalock et al', 'question': 'What open source library does the code in this text build on?', 'ans': 'The code builds on the open source ShrinkBench library introduced in (Blalock et al., 2020).'}, {'input': ', 2020), and a modi\x0ced version of the implementation of VGG11 provided\nin (Phan, 2021), where we changed the number of neurons in the \x0crst two layers to 128. The code for\nreproducing all experiments is available at https://github.com/marwash25/subpruning', 'question': 'What modifications were made to the implementation of VGG11 used in the study?', 'ans': 'The number of neurons in the first two layers were changed to 128.'}, {'input': "The code for\nreproducing all experiments is available at https://github.com/marwash25/subpruning .\nTo compute the gradients and activations used for pruning in ActGrad ,LayerActGrad , and\nour method's variants, we use four batches of size 128 of training images, i.e", 'question': 'Where can the code for reproducing all experiments be found?', 'ans': 'The code for reproducing all experiments can be found at https://github.com/marwash25/subpruning'}, {'input': "The code for\nreproducing all experiments is available at https://github.com/marwash25/subpruning .\nTo compute the gradients and activations used for pruning in ActGrad ,LayerActGrad , and\nour method's variants, we use four batches of size 128 of training images, i.e", 'question': 'Where can the code for reproducing all experiments be found?', 'ans': 'The code for reproducing all experiments can be found at https://github.com/marwash25/subpruning.'}, {'input': "com/marwash25/subpruning .\nTo compute the gradients and activations used for pruning in ActGrad ,LayerActGrad , and\nour method's variants, we use four batches of size 128 of training images, i.e., n= 512, which\ncorresponds to\x181% of the training data in MNIST and CIFAR10", 'question': 'How many batches of training images are used to compute the gradients and activations for pruning in ActGrad, LayerActGrad, and their variants?', 'ans': 'Four batches of size 128, which corresponds to 1% of the training data in MNIST and CIFAR10.'}, {'input': 'In International\nConference on Learning Representations , 2015.\nY Lecun, C Cortes, and C Burges. The mnist databaseof handwritten digits, 1998.\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel', 'question': 'What is the dataset used for training in this experiment?', 'ans': 'The MNIST and CIFAR10 datasets were used for training.'}, {'input': "18), which can be pruned almost entirely without a\x0becting the model's accuracy.\nHence, given a global budget, the per-layer budget selection assigns most of the budget to the \x0crst\nthree layers, which yield better results than when we are forced to prune a large fraction of early\nlayers", 'question': 'What is the global budget set to when pruning L layers?', 'ans': 'The global budget is set to k=PL/n, where n is the number of neurons/channels in layer L.'}, {'input': 'Unless otherwise speci\x0ced,\nwe use the per-layer budget selection method described in Section 5.2 for all the pruning methods.\nWe use a subset of the training set, of the same size as the validation set, as veri\x0ccation set for the\nbudget selection method', 'question': 'What is the per-layer budget selection method?', 'ans': 'The per-layer budget selection method is used to set the global budget to k=PL, where n is the number of neurons/channels in layer L.'}, {'input': 'Unless otherwise speci\x0ced,\nwe use the per-layer budget selection method described in Section 5.2 for all the pruning methods.\nWe use a subset of the training set, of the same size as the validation set, as veri\x0ccation set for the\nbudget selection method', 'question': 'What method is used for per-layer budget selection in pruning methods?', 'ans': 'The per-layer budget selection method described in Section 5.2 is used for all the pruning methods.'}, {'input': 'Unless otherwise speci\x0ced,\nwe use the per-layer budget selection method described in Section 5.2 for all the pruning methods.\nWe use a subset of the training set, of the same size as the validation set, as veri\x0ccation set for the\nbudget selection method', 'question': 'What is the purpose of using a verification set in the budget selection method?', 'ans': 'The purpose of using a verification set in the budget selection method is for fair comparison and to disentangle the benefit of using the pruning method from the benefit of reweighting.'}, {'input': 'For fair comparison and to disentangle the bene\x0ct of using our pruning\nmethod from the bene\x0ct of reweighting (Section 3.2), we report results with reweighting applied to\nall pruning methods, or none of them', 'question': 'What is the purpose of applying reweighting to all pruning methods?', 'ans': 'The purpose is to disentangle the benefit of using the pruning method from the benefit of reweighting.'}, {'input': 'Mathematics of Operations Research , 42(4):\n1197{1218, 2017.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned', 'question': 'What is the focus of the analysis?', 'ans': 'The focus of the analysis is on the more interesting results with reweighting, with the plots without reweighting mostly serving as a demonstration of the benefit of reweighting.'}, {'input': 'Though, we will focus our analysis on the more interesting\nresults with reweighting, with the plots without reweighting mostly serving as a demonstration of\nthe bene\x0ct of reweighting.\n9Results are averaged over \x0cve random runs, with standard deviations plotted as error bars', 'question': 'What is the focus of the analysis and why were plots without reweighting used?', 'ans': 'The focus of the analysis is on the more interesting results with reweighting, with the plots without reweighting mostly serving as a demonstration of the benefit of reweighting.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the focus of section 6.1?', 'ans': 'The focus of section 6.1 is on neuron pruning.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the focus of section 6.1?', 'ans': 'The focus of section 6.1 is on neuron pruning.'}, {'input': 'We pre-train LeNet model on MNIST for 200 epochs,\nwith a batch size of 128, using SGD with Nestrov momentum 0 :9 and a \x0cxed learning rate of\n1\x0210\x003. The resulting model achieves 97 :75% top-1 accuracy', 'question': 'What is the top-1 accuracy achieved by the LeNet model pre-trained on MNIST?', 'ans': 'The LeNet model pre-trained on MNIST achieves a top-1 accuracy of 97.75%.'}, {'input': 'The resulting model achieves 90 :11% top-1 accuracy. We prune two of\nthe three linear layers in VGG11, where n`=n`+1= 128 in the \x0crst layer, and n`= 128;n`+1= 10\nin the second one. We again do not prune the last classi\x0cer layer', 'question': 'What is the accuracy achieved by the resulting model?', 'ans': 'The resulting model achieves 97.75% top-1 accuracy.'}, {'input': 'Note that pruning is easier in this setting, as it is not necessary to have a network as large\nas VGG11 to achieve good performance on MNIST. This allows us to maintain an accuracy close to\nthe original model up to fraction 0 :05.\n6', 'question': 'What is the purpose of pruning in LeNet?', 'ans': 'The purpose of pruning in LeNet is to remove two of the three linear layers, while keeping the last classifier layer intact.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What does Figure 1 show in regards to the top-1 accuracy?', 'ans': 'Figure 1 shows the top-1 accuracy for different fractions of prunable neurons kept.'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which variant of the method performs the best and which one performs the worst?', 'ans': 'AsymInChange performs the best and LayerInChange performs the worst.'}, {'input': 'When both \x0cne-tuning and reweighting are used, all methods\nperform fairly similarly, except WeightNorm which consistently performs worst, and random\npruning methods which sometimes perform worst (see Figures 13 and 14)', 'question': 'What is the effect of reweighting on the performance of all methods?', 'ans': 'Reweighting significantly improves performance for all methods.'}, {'input': 'We also pre-train VGG11 model on CIFAR-10 for 200 epochs, with a batch size of 128, using Adam\nwith\x0c1= 0:9;\x0c2= 0:99, a weight decay of 5 \x0210\x004, and an initial learning rate of 1 \x0210\x003, dropped\nby 0:1 at epochs 100 and 150. The resulting model achieves 90 :11% top-1 accuracy', 'question': 'What is the top-1 accuracy achieved by the pre-trained VGG11 model on CIFAR-10?', 'ans': 'The pre-trained VGG11 model achieves 90.11% top-1 accuracy on CIFAR-10.'}, {'input': 'The resulting model achieves 90 :11% top-1 accuracy. We prune two of\nthe three linear layers in VGG11, where n`=n`+1= 128 in the \x0crst layer, and n`= 128;n`+1= 10\nin the second one. We again do not prune the last classi\x0cer layer', 'question': 'What is the top-1 accuracy achieved by the resulting model?', 'ans': 'The resulting model achieves 90.11% top-1 accuracy.'}, {'input': 'We \x0crst consider the same setup\nas in Section 6.2, but we prune four of the convolutional layers of VGG11 (features.0, 8, 11, 18),\ninstead of two. In these layers, n`= 64;256;512;512, andn`+1= 128;256;512;512, respectively', 'question': 'What is the value of n` in the first layer of the pruned VGG11 model?', 'ans': 'n`=128'}, {'input': '0, 8, 11, 18), instead of two. Figure 6 shows the\ntop-1 and top-5 accuracy for di\x0berent fractions of prunable channels kept. All variants of our method\nperform similarly here, and are again among the best performing methods', 'question': 'Which variant of the method is the best in terms of top-1 accuracy?', 'ans': 'LayerInChange'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which variant of the method performs the best in this setting?', 'ans': 'LayerInChange'}, {'input': 'The random methods perform surprisingly well here, even better than other\nbaselines. Though in all other settings we consider, they perform badly. As before, reweighting\nbene\x0cts all methods and reduces variance', 'question': 'How do the random methods perform in this setting compared to other baselines?', 'ans': 'The random methods perform surprisingly well here, even better than other baselines.'}, {'input': 'As before, reweighting\nbene\x0cts all methods and reduces variance.\nFigure 1: Top-1 Accuracy of di\x0berent neuron pruning methods on MNIST, after pruning two linear\nlayers in LeNet model, with di\x0berent fractions of remaining neurons (in log-scale), with (left) and\nwithout (right) reweighting', 'question': 'What is the effect of reweighting on different neuron pruning methods on MNIST?', 'ans': 'Reweighting benefits all methods and reduces variance.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the focus of section 6.2?', 'ans': 'Section 6.2 focuses on channel pruning.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the focus of section 6.2?', 'ans': 'The focus of section 6.2 is on channel pruning.'}, {'input': 'We prune two of\nthe three linear layers in VGG11, where n`=n`+1= 128 in the \x0crst layer, and n`= 128;n`+1= 10\nin the second one. We again do not prune the last classi\x0cer layer.\nFigure 2 shows the top-1 accuracy for di\x0berent fractions of prunable neurons kept', 'question': 'Which layers of the VGG11 model are pruned in this experiment?', 'ans': 'The first two of the eight convolution layers are pruned.'}, {'input': '1.\nWe prune the \x0crst two of the eight convolution layers of VGG11, where n`= 64;n`+1= 128 in the\n\x0crst layer,n`= 128;n`+1= 256 in the second one, and the kernel size is 3 \x023, i.e.,rh=rw= 3.\nWe present results with four convolution layers pruned in Appendix D', 'question': 'How many convolution layers were pruned in VGG11?', 'ans': 'The first two convolution layers were pruned.'}, {'input': '2', 'question': 'What does Figure 3 show?', 'ans': 'Figure 3 shows the top-1 accuracy for different fractions of prunable channels kept.'}, {'input': '2', 'question': 'What does Figure 3 show?', 'ans': 'Figure 3 shows the top-1 accuracy for different fractions of prunable channels kept.'}, {'input': 'The three variants\nof our method are among the best performing methods, though their performance is matched by\nLayerActGrad for most fractions, except very small ones. However, LayerActGrad performs\nworst in all other settings we consider', 'question': 'How does the performance of the three variants of the method compare to LayerActGrad?', 'ans': 'The three variants of the method are among the best performing methods, though their performance is matched by LayerActGrad for most fractions, except very small ones. However, LayerActGrad performs worst in all other settings we consider.'}, {'input': 'The three variants\nof our method are among the best performing methods, though their performance is matched by\nLayerActGrad for most fractions, except very small ones. However, LayerActGrad performs\nworst in all other settings we consider', 'question': 'Which variant is the best performing in all other settings except for LayerActGrad?', 'ans': 'SeqInChange is the best performing variant in all other settings except for LayerActGrad.'}, {'input': "Pruning convolution layers seems to a\x0bect performance more than pruning\nlinear layers, as we observe a signi\x0ccant drop in accuracy at fraction 0 :1 in Figure 3, which doesn't\nhappen until fraction 0 :05 in Figures 1 and 2", 'question': 'Which type of layers are more affected by pruning, convolution or linear layers?', 'ans': 'Convolution layers are more affected by pruning.'}, {'input': 'This can be explained by the fact that pruning\nchannels results in removing more weights than pruning neurons.\n6', 'question': 'What is the reason behind pruning channels instead of neurons?', 'ans': 'Pruning channels results in removing more weights than pruning neurons.'}, {'input': '6.3 Transfer learning\nIn this section, we consider a transfer learning setting, where given a large pre-trained network,\nour goal is to obtain a small model on a di\x0berent but related task to the one it was trained on', 'question': 'What is transfer learning?', 'ans': 'Transfer learning is a setting where a pre-trained network is used to obtain a small model on a different but related task to the one it was trained on.'}, {'input': '6.3 Transfer learning\nIn this section, we consider a transfer learning setting, where given a large pre-trained network,\nour goal is to obtain a small model on a di\x0berent but related task to the one it was trained on', 'question': 'What model was used for pre-training in Sections 6.1 and 6.2?', 'ans': 'The VGG11 model pretrained on CIFAR-10 was used for pre-training in Sections 6.1 and 6.2.'}, {'input': '1 and 6.2. We \x0cne-tune it for 10 epochs on MNIST\nwith the same setup used during pre-training, and obtain 98 :09% top-1 accuracy. We refer to the\nresulting model as VGG11-MNIST. For the per-layer budget selection, we use the layerwise accuracy\nmetricsP`(k`) computed on CIFAR-10', 'question': 'What is the accuracy of the VGG11-MNIST model after fine-tuning it for 10 epochs on MNIST?', 'ans': 'The VGG11-MNIST model obtained 98.09% top-1 accuracy after fine-tuning it for 10 epochs on MNIST.'}, {'input': 'We refer to the\nresulting model as VGG11-MNIST. For the per-layer budget selection, we use the layerwise accuracy\nmetricsP`(k`) computed on CIFAR-10. We prune the same two convolution layers as in Section 6.2.\nResults with four convolution layers pruned are presented in Appendix D.1', 'question': 'What is the name of the resulting model?', 'ans': 'The resulting model is named VGG11-MNIST.'}, {'input': 'Lemma A.4. Given a matrix D, vectory, and a vector of optimal regression coe\x0ecients xS(y)2\narg minsupp(x)\x12S1\n2ky\x00Dxk2\n2, we have for all S\x12V;i62S:\nxS[i(y) = (xS(y)\x00xS(di)\rS;i(y)) +\rS;i(y)1i2arg min\nsupp(x)\x12S[i1\n2ky\x00Dxk2\n2;\nwhere\rS;i(y)2arg min\r2R1\n2ky\x00RS(di)\rk2\n2', 'question': 'What is presented in Appendix D.1?', 'ans': 'Results with four convolution layers pruned.'}, {'input': '2', 'question': 'What does Figure 3 show?', 'ans': 'Figure 3 shows the top-1 accuracy of different channel pruning methods on CIFAR10, after pruning two convolution layers in VGG11 model, with different fractions of remaining channels (in log-scale), with and without reweighting.'}, {'input': 'variants of our method outperform other baselines signi\x0ccantly at fraction 0 :05 and slightly at\nfraction 0:075.AsymInChange is the best performing variant here, with the two others yielding\nmatching results. Here also reweighting improves the performance of all methods and reduces\nvariance', 'question': 'Which variant of the method is the best performing at fraction 0.05?', 'ans': 'AsymInChange'}, {'input': 'Similar to (Mariet and Sra, 2015), we\nobserve that reweighting provide a signi\x0ccant boost in performance not only to our method, but also\nto other baselines we consider. However unlike (Mariet and Sra, 2015), we only use a small fraction\nof the training data, around \x181% in our experiments', 'question': 'What is the benefit of reweighting in this setting?', 'ans': 'Reweighting improves the performance of all methods and reduces variance.'}, {'input': '2).\n•In all one shot pruning settings we considered, reweighting signi\x0ccantly improves performance\nfor all methods, and reduces variance in results.\n•The choice of how much to prune in each layer given a global budget can have a drastic e\x0bect\non performance, as illustrated in Appendix F', 'question': 'What is the advantage of pruning in this setting?', 'ans': 'Pruning is easier in this setting as it is not necessary to have a network as large as VGG11 to achieve good performance on MNIST.'}, {'input': '6.4 Discussion\nWe summarize our observations from the empirical results:\n•Our proposed pruning method outperforms common baselines like WeightNorm ,ActGrad ,\nRandom , and their layerwise variants, in various one shot pruning settings', 'question': 'How does the proposed pruning method compare to common baselines like WeightNorm, ActGrad, and Random?', 'ans': 'The proposed pruning method outperforms common baselines like WeightNorm, ActGrad, Random, and their layerwise variants, in various one shot pruning settings.'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which variant of the method usually performs the best?', 'ans': 'AsymInChange'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which variant of the method usually performs the best?', 'ans': 'AsymInChange'}, {'input': '2).\n•In all one shot pruning settings we considered, reweighting signi\x0ccantly improves performance\nfor all methods, and reduces variance in results.\n•The choice of how much to prune in each layer given a global budget can have a drastic e\x0bect\non performance, as illustrated in Appendix F', 'question': 'What is the effect of reweighting in one shot pruning settings?', 'ans': 'Reweighting significantly improves performance for all methods and reduces variance in results.'}, {'input': '2).\n•In all one shot pruning settings we considered, reweighting signi\x0ccantly improves performance\nfor all methods, and reduces variance in results.\n•The choice of how much to prune in each layer given a global budget can have a drastic e\x0bect\non performance, as illustrated in Appendix F', 'question': 'What is the effect of reweighting in one shot pruning settings?', 'ans': 'Reweighting significantly improves performance for all methods and reduces variance in results.'}, {'input': 'We further highlight the e\x0bect of per-layer budget selection on performance in Appendix F.\nReweighting again helps all methods in both pruning tasks considered here', 'question': 'What is illustrated in Appendix E regarding performance?', 'ans': 'Fine-tuning boosts performance even more than reweighting.'}, {'input': '•Fine-tuning boosts performance even more than reweighting, as illustrated in Appendix E.\nThough reweighting often still helps even when \x0cne-tuning is used. Hence, both \x0cne-tuning\nand reweighting should be used when possible, i.e., in settings where data is not limited', 'question': 'What is the recommendation for using fine-tuning and reweighting?', 'ans': 'Both fine-tuning and reweighting should be used when possible, in settings where data is not limited.'}, {'input': '•The choice of how much to prune in each layer given a global budget can have a drastic e\x0bect\non performance, as illustrated in Appendix F.\n•Fine-tuning boosts performance even more than reweighting, as illustrated in Appendix E.\nThough reweighting often still helps even when \x0cne-tuning is used', 'question': 'What is the importance of pruning method in small models when both fine-tuning and reweighting are used?', 'ans': 'The choice of pruning method is less important since all methods perform fairly similarly, except WeightNorm and the random selection methods.'}, {'input': "Note that the results in Figures 5 and 6 are better than their counterparts in Figures 3 and 4,\nwhere we prune less layers. This is because the current four layers being pruned include a late\n25layer (features.18), which can be pruned almost entirely without a\x0becting the model's accuracy", 'question': 'What does Figure 4 show?', 'ans': 'Figure 4 shows the Top-1 Accuracy of different channel pruning methods on MNIST, after pruning two convolution layers in VGG11-MNIST model, with different fractions of remaining channels (in log-scale), with and without reweighting.'}, {'input': 'However, current structured pruning methods are highly empirical in nature,\ndo not provide any theoretical guarantees, and often require \x0cne-tuning, which makes them\ninapplicable in the limited-data regime', 'question': 'What is the proposed pruning method based on?', 'ans': 'The proposed pruning method is based on submodular optimization.'}, {'input': 'By\ncasting the layerwise subset selection problem as a weakly submodular optimization problem, we are\nable to use the Greedy algorithm to provably approximate it. Empirically, our method outperforms\ncommon baselines on various neuron and channel one-shot pruning tasks', 'question': 'What is the approach used to solve the layerwise subset selection problem?', 'ans': 'The layerwise subset selection problem is solved by casting it as a weakly submodular optimization problem.'}, {'input': 'slightly worst results, as in Figure 13; our methods have slightly lower accuracy when reweighting is\nused, for fractions larger than 0 :05', 'question': 'What is the result of the method described in the text?', 'ans': 'Empirically, the method outperforms common baselines on various neuron and channel one-shot pruning tasks.'}, {'input': 'Geo\x0brey Hinton, Oriol Vinyals, and Je\x0b Dean. Distilling the knowledge in a neural network. Neural\nInformation Processing Systems (NeurIPS) Workshops , 2015.\nTorsten Hoe\rer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste', 'question': 'Who is being acknowledged in this text?', 'ans': 'Stefanie Jegelka, Debadeepta Dey, Jose Gallego-Posada and Boris Knyazev'}, {'input': 'We also acknowledge the MIT SuperCloud\nand Lincoln Laboratory Supercomputing Center ( supercloud.mit.edu ), Compute Canada ( www.\ncomputecanada.ca ), WestGrid (www.westgrid', 'question': 'What are the two supercomputing centers acknowledged in the text?', 'ans': 'The MIT SuperCloud and Lincoln Laboratory Supercomputing Center, and Compute Canada and WestGrid.'}, {'input': 'mit.edu ), Compute Canada ( www.\ncomputecanada.ca ), WestGrid (www.westgrid.ca), and the Mila IDT team for providing HPC\nresources that have contributed to the research results reported within this paper. This research was\npartially supported by the Canada CIFAR AI Chair Program', 'question': 'Which organizations provided HPC resources for the research reported in the paper?', 'ans': 'The organizations that provided HPC resources for the research reported in the paper are MIT.edu, Compute Canada, WestGrid, and the Mila IDT team.'}, {'input': 'This research was\npartially supported by the Canada CIFAR AI Chair Program. Simon Lacoste-Julien is a CIFAR\nAssociate Fellow in the Learning Machines & Brains program.\nReferences\nAndrew An Bian, Joachim M Buhmann, Andreas Krause, and Sebastian Tschiatschek', 'question': 'What program partially supported the research mentioned in the text?', 'ans': 'The Canada CIFAR AI Chair Program.'}, {'input': 'This research was\npartially supported by the Canada CIFAR AI Chair Program. Simon Lacoste-Julien is a CIFAR\nAssociate Fellow in the Learning Machines & Brains program.\nReferences\nAndrew An Bian, Joachim M Buhmann, Andreas Krause, and Sebastian Tschiatschek', 'question': "What is Simon Lacoste-Julien's role in the Learning Machines & Brains program?", 'ans': 'Simon Lacoste-Julien is a CIFAR Associate Fellow in the Learning Machines & Brains program.'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': 'Guarantees for greedy maximization of non-submodular functions with applications'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Guarantees for greedy maximization of non-submodular functions with applications' and it was published in the Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 498-507, by JMLR.org in 2017. The authors are Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': 'In Proceedings of the 34th International Conference on Machine Learning-Volume 70'}, {'input': 'In Proceedings of the\n34th International Conference on Machine Learning-Volume 70 , pages 498{507. JMLR. org, 2017.\nDavis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state\nof neural network pruning? arXiv preprint arXiv:2003.03033 , 2020', 'question': 'What is the title of the paper written by Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag?', 'ans': "The title of the paper is 'What is the state of neural network pruning?'"}, {'input': "03033 , 2020.\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings\nof the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ,\nKDD '06, page 535{541, New York, NY, USA, 2006. Association for Computing Machinery. ISBN\n1595933395", 'question': 'What is the title of the paper presented by Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil in the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?', 'ans': 'Model compression.'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the publication with the ISBN 1595933395 and doi 10.1145/1150402.1150464?', 'ans': 'The title of the publication is Association for Computing Machinery.'}, {'input': 'Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26.\nCurran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/\n7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf', 'question': 'What is the title of the paper written by Sebastian Buschjager, Philipp-Jan Honysz, and Katharina Morik?', 'ans': 'Very fast streaming submodular function maximization'}, {'input': 'Maxwell D Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. arXiv\npreprint arXiv:1412.1442 , 2014.\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural\nnetworks with binary weights during propagations', 'question': 'What is the title of the paper by Maxwell D Collins and Pushmeet Kohli?', 'ans': 'Memory bounded deep convolutional networks'}, {'input': 'Maxwell D Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. arXiv\npreprint arXiv:1412.1442 , 2014.\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural\nnetworks with binary weights during propagations', 'question': 'What is the title of the paper by Maxwell D Collins and Pushmeet Kohli?', 'ans': 'Memory bounded deep convolutional networks'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the paper by Courbariaux, Bengio, and David?', 'ans': 'Binaryconnect: Training deep neural networks with binary weights during propagations.'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': 'Binaryconnect: Training deep neural networks with binary weights during propagations.'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper written by Das and Kempe?', 'ans': 'Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection.'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and who are the authors?', 'ans': "The title of the paper is 'Predicting parameters in deep learning' and the authors are Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas."}, {'input': 'Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26', 'question': 'What is the title of the paper written by Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas?', 'ans': 'Predicting parameters in deep learning'}, {'input': 'arXiv preprint arXiv:1102.3975 , 2011.\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Q', 'question': "What is the title of the book that contains the article 'Predicting parameters in deep learning'?", 'ans': 'Advances in Neural Information Processing Systems'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the book and who are the editors?', 'ans': 'The title of the book is Advances in Neural Information Processing Systems and the editors are M. Bottou, Z. Welling, Z. Ghahramani, and K. Q. Weinberger.'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and who are the authors?', 'ans': "The title of the paper is 'Exploiting linear structure within convolutional networks for efficient evaluation' and the authors are Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'Exploiting linear structure within convolutional networks for efficient evaluation' and it was published in Advances in neural information processing systems in 2014."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Exploiting linear structure within convolutional networks for efficient evaluation' and it was published in Advances in neural information processing systems in 2014."}, {'input': 'Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26.\nCurran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/\n7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf', 'question': 'What is the title of the book mentioned in the text?', 'ans': 'Advances in neural information processing systems'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Combinatorial penalties: Structure preserved by convex relaxations' and it was published in the Proceedings of the 21st International Conference on Artificial Intelligence and Statistics in 2018."}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the conference mentioned in the text?', 'ans': 'Proceedings of the 21st International Conference on Artificial Intelligence and Statistics'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the first paper mentioned in the text?', 'ans': 'Restricted strong convexity implies weak submodularity'}, {'input': 'arXiv preprint arXiv:1805.10352 ,\n2018.\nMaxim Sviridenko, Jan Vondr\x13 ak, and Justin Ward. Optimal approximation for submodular and\nsupermodular optimization with bounded curvature. Mathematics of Operations Research , 42(4):\n1197{1218, 2017', 'question': 'What is the title of the paper with arXiv ID 1612.00804?', 'ans': 'Compressing deep convolutional networks using vector quantization.'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Compressing deep convolutional networks using vector quantization' and it was published as an arXiv preprint with the identifier arXiv:1412.6115 in the year 2014."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': 'Optimal brain surgeon and general network pruning'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': "The title of the paper mentioned in the text is 'Reshaping deep neural network for fast decoding by node-pruning'."}, {'input': 'In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing ,\npages 6655{6659, 2013. doi: 10.1109/ICASSP.2013.6638949.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition', 'question': 'What is the title of the paper presented by Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu in the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing?', 'ans': 'Reshaping deep neural network for fast decoding by node-pruning'}, {'input': 'IEEE, 2014.\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.\nInProceedings of the IEEE international conference on computer vision , pages 1389{1397, 2017.\nGeo\x0brey Hinton, Oriol Vinyals, and Je\x0b Dean. Distilling the knowledge in a neural network', 'question': "What is the focus of the paper 'Channel pruning for accelerating very deep neural networks'?", 'ans': 'The focus of the paper is on channel pruning for accelerating very deep neural networks.'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the paper written by Yihui He, Xiangyu Zhang, and Jian Sun?', 'ans': 'Channel pruning for accelerating very deep neural networks.'}, {'input': 'InProceedings of the IEEE international conference on computer vision , pages 1389{1397, 2017.\nGeo\x0brey Hinton, Oriol Vinyals, and Je\x0b Dean. Distilling the knowledge in a neural network. Neural\nInformation Processing Systems (NeurIPS) Workshops , 2015', 'question': "What is the name of the conference where the paper 'Distilling the knowledge in a neural network' was presented?", 'ans': 'Neural Information Processing Systems (NeurIPS) Workshops'}, {'input': 'Geo\x0brey Hinton, Oriol Vinyals, and Je\x0b Dean. Distilling the knowledge in a neural network. Neural\nInformation Processing Systems (NeurIPS) Workshops , 2015.\nTorsten Hoe\rer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste', 'question': 'What is the title of the paper written by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in 2015?', 'ans': 'Distilling the knowledge in a neural network'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Distilling the knowledge in a neural network' and it was published in the Neural Information Processing Systems (NeurIPS) Workshops in 2015."}, {'input': 'Torsten Hoe\rer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in\ndeep learning: Pruning and growth for e\x0ecient inference and training in neural networks. arXiv\npreprint arXiv:2102.00554 , 2021.\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman', 'question': 'What is the title of the paper written by Torsten Hoefer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste?', 'ans': 'Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks'}, {'input': 'Sparsity in\ndeep learning: Pruning and growth for e\x0ecient inference and training in neural networks. arXiv\npreprint arXiv:2102.00554 , 2021.\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\nwith low rank expansions', 'question': "What is the purpose of the paper 'Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks'?", 'ans': 'The purpose of the paper is to discuss pruning and growth techniques for efficient inference and training in neural networks.'}, {'input': 'Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26.\nCurran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/\n7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf', 'question': 'What is the title of the paper mentioned in the preprint arXiv:2102.00554?', 'ans': 'Speeding up convolutional neural networks with low rank expansions'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Speeding up convolutional neural networks with low rank expansions' and it was published in the Proceedings of the British Machine Vision Conference by BMVA Press in 2014."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the publication that this text is from?', 'ans': 'Proceedings of the British Machine Vision Conference'}, {'input': 'Speeding up convolutional neural networks\nwith low rank expansions. In Proceedings of the British Machine Vision Conference . BMVA Press,\n2014. doi: http://dx.doi.org/10.5244/C.28.88.\n14Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin', 'question': 'What is the title of the paper written by Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin?', 'ans': 'Compression of deep convolutional neural networks for fast and low power mobile applications'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the topic of the paper with arXiv ID 1511.06530?', 'ans': 'The topic of the paper with arXiv ID 1511.06530 is compression of deep convolutional neural networks for fast and low power mobile applications.'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the preprint with the identifier arXiv:1511.06530?', 'ans': 'The title of the preprint is not provided in the given text.'}, {'input': 'arXiv\npreprint arXiv:1511.06530 , 2015.\nAlex Krizhevsky, Geo\x0brey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nAndrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen Blankevoort, and Max\nWelling', 'question': 'What is the title of the paper written by Alex Krizhevsky, Geoffrey Hinton, and others in 2009?', 'ans': 'Learning multiple layers of features from tiny images'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and who are the authors?', 'ans': "The title of the paper is 'Taxonomy and evaluation of structured compression of convolutional neural networks' and the authors are Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen Blankevoort, and Max Welling."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'Taxonomy and evaluation of structured compression of convolutional neural networks' and it was published as an arXiv preprint with the identifier arXiv:1912.09802 in 2019."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and who are the authors?', 'ans': "The title of the paper is 'Speeding-up convolutional neural networks using fine-tuned cp-decomposition' and the authors are Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V Oseledets, and Victor S Lempitsky."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and where was it presented?', 'ans': "The title of the paper is 'Speeding-up convolutional neural networks using fine-tuned cp-decomposition' and it was presented at the International Conference on Learning Representations in 2015."}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the name of the conference mentioned in the text?', 'ans': 'International Conference on Learning Representations'}, {'input': 'In International\nConference on Learning Representations , 2015.\nY Lecun, C Cortes, and C Burges. The mnist databaseof handwritten digits, 1998.\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel', 'question': 'What is the title of the paper written by Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel?', 'ans': 'Backpropagation applied to handwritten zip code recognition.'}, {'input': '2', 'question': 'What is the topic of the text?', 'ans': 'The topic of the text is backpropagation applied to handwritten zip code recognition.'}, {'input': 'Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf', 'question': 'What is the title of the paper written by Yann LeCun, John Denker, and Sara Solla?', 'ans': 'Optimal brain damage'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the book edited by Touretzky in 1990?', 'ans': 'Advances in Neural Information Processing Systems'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper published in Morgan-Kaufmann in 1990 and where can it be found?', 'ans': 'The title of the paper is not provided in the given text. However, the URL to access the paper is https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf.'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article written by Benny Lehmann, Daniel Lehmann, and Noam Nisan?', 'ans': 'Combinatorial auctions with decreasing marginal utilities'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'Pruning filters for efficient convnets' and it was published in the Conference Track Proceedings of the 5th International Conference on Learning Representations, ICLR 2017, in Toulon, France."}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the paper presented at the 5th International Conference on Learning Representations in 2017?', 'ans': 'Pruning filters for efficient convnets'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the paper presented by Jian-Hao Luo, Jianxin Wu, and Weiyao Lin in the IEEE international conference on computer vision in 2017?', 'ans': 'Thinet: A filter level pruning method for deep neural network compression.'}, {'input': 'Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression using determinantal\npoint processes. arXiv preprint arXiv:1511.05077 , 2015.\nKris McGu\x0ee and Alex Newhouse. The radicalization risks of gpt-3 and advanced neural language\nmodels. arXiv preprint arXiv:2009.06807 , 2020', 'question': 'What is the title of the paper by Zelda Mariet and Suvrit Sra?', 'ans': 'Diversity networks: Neural network compression using determinantal point processes'}, {'input': '2', 'question': 'What are the titles of the two articles mentioned in the text?', 'ans': "The titles of the two articles mentioned in the text are 'Diversity networks: Neural network compression using determinantal point processes' and 'The radicalization risks of gpt-3 and advanced neural language models'."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper by Kris McGuire and Alex Newhouse?', 'ans': 'The radicalization risks of gpt-3 and advanced neural language models.'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the paper presented in the AAAI Conference on Artificial Intelligence in 2015?', 'ans': 'Lazier than lazy greedy'}, {'input': 'Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26.\nCurran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/\n7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf', 'question': 'What is the title of the paper published in the Proceedings of the AAAI Conference on Artificial Intelligence in 2015?', 'ans': 'Lazier than lazy greedy.'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper written by Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz?', 'ans': 'Pruning convolutional neural networks for resource efficient inference'}, {'input': 'ICLR , 2017.\nBalas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on computing ,\n24(2):227{234, 1995.\nG.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. An analysis of approximations for maximizing\nsubmodular set functions | I', 'question': 'What is the title of the paper by Balas Kausik Natarajan published in the SIAM journal on computing in 1995?', 'ans': 'Sparse approximate solutions to linear systems'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the journal and the year of publication for the first article mentioned in the text?', 'ans': 'The title of the journal is SIAM journal on computing and the year of publication is 1995.'}, {'input': 'A. Wolsey, and M.L. Fisher. An analysis of approximations for maximizing\nsubmodular set functions | I. Mathematical Programming , 14(1):265{294, 1978.\n15Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer', 'question': 'What is the title of the article written by A. Wolsey and M.L. Fisher?', 'ans': 'An analysis of approximations for maximizing submodular set functions'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'Mathematical Programming' and it was published in 1978."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and who is the author?', 'ans': "The title of the paper is 'Automatic differentiation in pytorch' and the author is Huy Phan."}, {'input': 'Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran.\nLow-rank matrix factorization for deep neural network training with high-dimensional output\ntargets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing ,\npages 6655{6659, 2013', 'question': 'What is the title of the paper written by Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran?', 'ans': "The title of the paper is 'Low-rank matrix factorization for deep neural network training with high-dimensional output targets'."}, {'input': 'In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing ,\npages 6655{6659, 2013. doi: 10.1109/ICASSP.2013.6638949.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition', 'question': 'What is the title of the paper presented at the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing?', 'ans': 'Low-rank matrix factorization for deep neural network training with high-dimensional output targets.'}, {'input': 'In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing ,\npages 6655{6659, 2013. doi: 10.1109/ICASSP.2013.6638949.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition', 'question': 'What is the title of the paper presented at the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing?', 'ans': "The title of the paper presented at the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing is 'Very deep convolutional networks for large-scale image recognition'."}, {'input': '2013.6638949.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015', 'question': 'What is the title of the paper written by Karen Simonyan and Andrew Zisserman?', 'ans': 'Very deep convolutional networks for large-scale image recognition.'}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the conference where the paper was presented?', 'ans': '3rd International Conference on Learning Representations, ICLR 2015'}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper and who are the authors?', 'ans': "The title of the paper is 'Data-free parameter pruning for deep neural networks' and the authors are Suraj Srinivas and R. Venkatesh Babu."}, {'input': 'Venkatesh Babu. Data-free parameter pruning for deep neural networks. In\nProceedings of the British Machine Vision Conference (BMVC) , pages 31.1{31.12. BMVA Press,\nSeptember 2015.\nJiahao Su, Jingling Li, Bobby Bhattacharjee, and Furong Huang', 'question': "What is the title of Venkatesh Babu's paper and where was it published?", 'ans': "The title of Venkatesh Babu's paper is 'Data-free parameter pruning for deep neural networks' and it was published in the Proceedings of the British Machine Vision Conference (BMVC) in September 2015 by BMVA Press."}, {'input': 'In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings ,\n2015. URL http://arxiv.org/abs/1409.1556 .\nSuraj Srinivas and R. Venkatesh Babu', 'question': 'What is the title of the conference where the paper was presented?', 'ans': 'Proceedings of the British Machine Vision Conference (BMVC)'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'Tensorial neural networks: Generalization of neural networks and application to model compression' and it was published as an arXiv preprint in September 2015 by BMVA Press."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Tensorial neural networks: Generalization of neural networks and application to model compression' and it was published as an arXiv preprint with the identifier arXiv:1805.10352 in 2018."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': 'Optimal approximation for submodular and supermodular optimization with bounded curvature.'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Optimal approximation for submodular and supermodular optimization with bounded curvature' and it was published in Mathematics of Operations Research, volume 42, issue 4, pages 1197-1218 in 2017. The authors are Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned' and it was published in Mathematics of Operations Research, volume 42, issue 4, pages 1197-1218 in 2017."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned' and it was published in the Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics in 2019."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper presented at the 57th Annual Meeting of the Association for Computational Linguistics?', 'ans': 'The title of the paper presented at the 57th Annual Meeting of the Association for Computational Linguistics is not provided in the given text.'}, {'input': 'In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics , pages 5797{5808, 2019.\nMao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks\nprovably exist: Pruning via greedy forward selection. ICML , 2020', 'question': 'What is the title of the paper written by Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu?', 'ans': 'Good subnetworks provably exist: Pruning via greedy forward selection'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the paper and where was it published?', 'ans': "The title of the paper is 'Accelerating very deep convolutional networks for classification and detection' and it was published in IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 38, issue 10, pages 1943-1955, in 2016. The paper also has a DOI of 10.1109/TPAMI.2015.2502579."}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the article and where was it published?', 'ans': "The title of the article is 'IEEE Transactions on Pattern Analysis and Machine Intelligence' and it was published in 2016."}, {'input': 'Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\n55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf', 'question': 'What is the title of the paper mentioned in the text?', 'ans': 'Discrimination-aware channel pruning for deep neural networks'}, {'input': 'Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26.\nCurran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/\n7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf', 'question': 'What is the title of the book chapter this text is from?', 'ans': 'Discrimination-aware channel pruning for deep neural networks'}, {'input': 'Touretzky, editor, Advances\nin Neural Information Processing Systems , volume 2. Morgan-Kaufmann, 1990. URL https:\n//proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.\npdf.\nBenny Lehmann, Daniel Lehmann, and Noam Nisan', 'question': 'What is the title of the book and who are the editors?', 'ans': 'The title of the book is Advances in Neural Information Processing Systems and the editors are K. Larochelle, N. Grauman, N. Cesa-Bianchi, and R. Garnett.'}, {'input': '17LetS0=M(S);L0=M(L),I0=M(i), andk0=rhrwk. As before, we can write G(S) =F(S0) =Pn`+1\nm=1Fm(S0) :=`m(0)\x00min supp( ~wm)\x12S0`m(~wm), where`m(~wm) =kA`w`+1\nm\x00A`~wmk2\n2. We denote\nby\x16kand\x17kthe RSC and RSM parameters of `m, given by\x16k=minkzk2=1;kzk0\x14kkA`zk2\n2, and\n\x17k= maxkzk2=1;kzk0\x14kkA`zk2\n2', 'question': 'What is the function G(S) defined as?', 'ans': 'G(S) = F(M(S))'}, {'input': 'We denote by ~F(S) the objective\ncorresponding to the asymmetric formulation introduced in Section 5.1, i.e., ~F(S) =kA`W`+1k2\nF\x00\nmin ~W`+12Rn`\x02n`+1kA`W`+1\x00B`\nS~W`+1k2\nF, and similarly ~G(S) = ~F(M(S)), whereMmaps each\nchannel to its corresponding columns in A`', 'question': 'What is the objective denoted by ~F(S)?', 'ans': 'The objective denoted by ~F(S) corresponds to the asymmetric formulation introduced in Section 5.1.'}, {'input': 'We introduce some notation that will be used throughout the Appendix. Given any matrix D\nand vector y, we denote by xS(y)2arg minsupp(x)\x12S1\n2ky\x00Dxk2\n2the vector of optimal regression\ncoe\x0ecients, and by projS(y) =DxS(y),RS(y) =y\x00projS(y) the corresponding projection and\nresidual.\nA', 'question': 'What is the notation introduced in the Appendix?', 'ans': 'The notation introduced in the Appendix is M, which maps each channel to its corresponding columns in A`.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the main result of this section?', 'ans': 'The main result of this section is the proof that F;G, and their asymmetric variants ~F;~G are all non-decreasing weakly submodular functions.'}, {'input': '4 Pruning regular regions of neurons\nIn this section, we discuss how to adapt our approach to pruning regular regions of neurons. This\nis easily achieved by mapping any set of regular regions to the corresponding set of neurons, then\napplying the same method in Section 3', 'question': 'What is the focus of this section?', 'ans': 'The focus of this section is to prove that F;G, and their asymmetric variants ~F;~G are all non-decreasing weakly submodular functions.'}, {'input': 'We start by reviewing the de\x0cnition of restricted smoothness (RSM) and\nrestricted strong convexity(RSC).\nDe\x0cnition A.1 (RSM/RSC)', 'question': 'What is the definition of RSM/RSC?', 'ans': 'The definition of RSM/RSC is restricted smoothness and restricted strong convexity.'}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What is Proposition 3.1 about?', 'ans': 'Proposition 3.1 is about a normalized non-decreasing U;k-weakly submodular function.'}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What can be said about the function F mentioned in Proposition 4.1?', 'ans': 'The function F is also U;k-weakly submodular, normalized, and non-decreasing.'}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What is Proposition 4.1 about?', 'ans': 'Proposition 4.1 is about a normalized non-decreasing U;k-weakly submodular function G, and provides a proof related to it.'}, {'input': 'Gis\rU;k-weakly submodular i\x0b Fsatis\x0ces\n\rU;kF(M(S)jM(L))\x14X\ni2SF(M(i)jM(L));\nfor every two disjoint sets L;S\x12V`, such thatL\x12U;jSj\x14k. We extend the relation established in\n(Elenberg et al', 'question': 'What relation is extended in the given text?', 'ans': 'The relation established in (Elenberg et al., 2016) between weak submodularity and RSC/RSM parameters is extended in the given text.'}, {'input': 'We denote\nby\x16kand\x17kthe RSC and RSM parameters of `m, given by\x16k=minkzk2=1;kzk0\x14kkA`zk2\n2, and\n\x17k= maxkzk2=1;kzk0\x14kkA`zk2\n2. To simplify notation, we use xS:=xS(A`w`+1\nm)', 'question': 'What do the symbols k and xS represent in the given text?', 'ans': "k represents the RSC and RSM parameters of 'm', while xS is a simplified notation used in the text."}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What are Proposition 3.1 and Proposition 4.1?', 'ans': 'Proposition 3.1 and Proposition 4.1 apply to the asymmetric variants, using exactly the same proofs.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What does this section investigate?', 'ans': 'This section investigates the cost of applying Greedy with F;G and their asymmetric variants ~F;~G.'}, {'input': '2).\nWe evaluate all three variants in our experiments. As expected, AsymInChange usually performs\nthe best, and LayerInChange the worst. But in some settings, LayerInChange yields the best\nresults (see Fig. 2).\n5', 'question': 'What does this section investigate and what are the variants used?', 'ans': 'This section investigates the cost of applying Greedy with F;G and their asymmetric variants ~F;~G.'}, {'input': 'To that end, we need the following key lemmas showing how to update the least squares\nsolutions and the function values after adding one or more elements.\nLemma A.4', 'question': 'What is the purpose of Lemma A.4?', 'ans': 'The purpose of Lemma A.4 is to show how to update the least squares solutions and the function values after adding one or more elements.'}, {'input': 'Lemma A.4. Given a matrix D, vectory, and a vector of optimal regression coe\x0ecients xS(y)2\narg minsupp(x)\x12S1\n2ky\x00Dxk2\n2, we have for all S\x12V;i62S:\nxS[i(y) = (xS(y)\x00xS(di)\rS;i(y)) +\rS;i(y)1i2arg min\nsupp(x)\x12S[i1\n2ky\x00Dxk2\n2;\nwhere\rS;i(y)2arg min\r2R1\n2ky\x00RS(di)\rk2\n2', 'question': 'What is the purpose of the matrix D, vector y, and vector of optimal regression coefficients xS(y) in the given lemma?', 'ans': 'They are used to find the optimal regression coefficients xS[i(y)] for all S in V and i not in S.'}, {'input': 'Hence, projS[i(y) =projS(y) +projRS(di)(y), where\nprojRS(di)(y) =RS(di)\rS;i(y)', 'question': 'What is the formula for projS[i(y)]?', 'ans': 'projS[i(y)] = projS(y) + projRS(di)(y)'}, {'input': 'Similarly, for I\x12VnS, letRS(DI)be the matrix with columns RS(di),xS(DI)the matrix with\ncolumnsxS(di), and\rS;I(y)2arg min\r2RjIj1\n2ky\x00RS(DI)\rk2\n2, then\nxS[I(y) = (xS(y)\x00xS(DI)\rS;I(y)) +eI\rS;I(y)2arg min\nsupp(x)\x12S[I1\n2ky\x00Dxk2\n2;\nwhereeI2RjVj\x02jIjis the matrix with [eI]i;i= 1for alli2I, and 0elsewhere', 'question': 'What is the purpose of the matrix xS[I(y)]?', 'ans': 'The matrix xS[I(y)] is used to minimize the value of the expression 2ky ||Dx||^2, where x is a vector and D is a matrix.'}, {'input': 'Hence, projS[i(y) =projS(y) +projRS(di)(y), where\nprojRS(di)(y) =RS(di)\rS;i(y)', 'question': 'What is the equation for projS[I(y)]?', 'ans': 'projS[I(y)] = projS(y) + projRS(DI)(y)'}, {'input': 'Proof. By optimality conditions, we have:\nD>\nS(DSxS(y)\x00y) = 0 (6)\nD>\nS(DSxS(di)\x00di) = 0)\x00D>\nSRS(di) = 0 (7)\nRS(di)>(RS(di)\rS;i(y)\x00y) = 0 (8)\nWe prove that ^xS[i(y) = (xS(y)\x00xS(di)\rS;i(y)) +\rS;i(y)1isatis\x0ces the optimality conditions on\nxS[i(y), hence ^xS[i(y) =xS[i(y)', 'question': 'What are the optimality conditions mentioned in the text?', 'ans': 'The optimality conditions mentioned in the text are: D> S(DSxS(y) y) = 0 (6), D> S(DSxS(di) di) = 0) D> SRS(di) = 0 (7), and RS(di)>(RS(di) S;i(y) y) = 0 (8).'}, {'input': 'Proof. We prove the claim for the case where we add several elements. The case where we add a\nsingle element then follows as a special case', 'question': 'What is the proof for the case where we add multiple indices at once?', 'ans': 'The proof for the case where we add multiple indices at once follows similarly.'}, {'input': 'Proof. SettingB`=A`in Lemma A.5, we get\n\x0bk\x15minkzk2=1k(A`W`+1)>zk2\n2\nmaxkzk2=1k(A`W`+1)>zk2\n2\n23To obtain the second lower bound, we note that by Lemma A', 'question': 'What is Lemma A.5 about?', 'ans': 'Lemma A.5 is about marginal gain and how to calculate it.'}, {'input': 'We can write the marginal gain of adding itoSw.r.t ~Fas:\n~F(ijS) =n`+1X\nm=1kprojRS(b`\ni)(A`)w`+1\nmk2\n2;\n19where projRS(b`\ni)(A`)is the matrix with columns projRS(b`\ni)(a`\nj)for allj2V`', 'question': 'What is the formula for the marginal gain of adding itoSw.r.t ~Fas?', 'ans': '~F(ijS) =n`+1X m=1kprojRS(b`i)(A`)w`+1 mk2 2; 19where projRS(b`i)(A`)is the matrix with columns projRS(b`i)(a`j)for allj2V`'}, {'input': 'r.t ~Fas:\n~F(ijS) =n`+1X\nm=1kprojRS(b`\ni)(A`)w`+1\nmk2\n2;\n19where projRS(b`\ni)(A`)is the matrix with columns projRS(b`\ni)(a`\nj)for allj2V`. Similarly, for all\nS\x12V`;I\x12V`nS, letRS(B`\nI) =B`\nI\x00projS(B`\nI),projRS(B`\nI)(y) =RS(B`\nI)\rS;I(y)with\rS;I(y)2\narg min\r2RjIjky\x00RS(B`\nI)\rk2\n2', 'question': 'What is the purpose of the matrix projRS(b`i)(A`)?', 'ans': 'The matrix projRS(b`i)(A`) has columns projRS(b`i)(a`j) for all j2V`, and it is used to calculate RS(B`I) = B`I projRS(b`I) and projRS(B`I)(y) = RS(B`I) S;I(y) with S;I(y)2 arg min 2RjIjky-RS(B`I)k2.'}, {'input': 'Similarly, for all\nS\x12V`;I\x12V`nS, letRS(B`\nI) =B`\nI\x00projS(B`\nI),projRS(B`\nI)(y) =RS(B`\nI)\rS;I(y)with\rS;I(y)2\narg min\r2RjIjky\x00RS(B`\nI)\rk2\n2. We can write the marginal gain of adding ItoSw.r', 'question': 'What is the marginal gain of adding I to Sw.r?', 'ans': 'The marginal gain of adding I to Sw.r can be written as 2RjIjky - RS(B`I)k2.'}, {'input': 'We can write the marginal gain of adding itoSw.r.t ~Fas:\n~F(ijS) =n`+1X\nm=1kprojRS(b`\ni)(A`)w`+1\nmk2\n2;\n19where projRS(b`\ni)(A`)is the matrix with columns projRS(b`\ni)(a`\nj)for allj2V`', 'question': 'What is the formula for the marginal gain of adding ItoSw with respect to ~Fas?', 'ans': '~F(IjS) =n`+1X m=1kprojRS(B`I)(A`)w`+1 mk2 2;'}, {'input': 'Proof. We prove the claim for the case where we add several elements. The case where we add a\nsingle element then follows as a special case', 'question': "What does the author mean by 'The case where we add a single element then follows as a special case'?", 'ans': 'The author means that the proof for adding a single element is a simpler version of the proof for adding several elements.'}, {'input': 'Proof. SettingB`=A`in Lemma A.5, we get\n\x0bk\x15minkzk2=1k(A`W`+1)>zk2\n2\nmaxkzk2=1k(A`W`+1)>zk2\n2\n23To obtain the second lower bound, we note that by Lemma A', 'question': 'What is the reason for the second to last equality in the given text?', 'ans': 'The second to last equality holds because yprojS(y)projRS(BI)(y) and projRS(BI)(y0) are orthogonal by optimality conditions.'}, {'input': 'To that end, we need the following key lemmas showing how to update the least squares\nsolutions and the function values after adding one or more elements.\nLemma A.4', 'question': 'What is the purpose of Lemma A.6?', 'ans': 'The purpose of Lemma A.6 is not provided in the given text.'}, {'input': 'To that end, we need the following key lemmas showing how to update the least squares\nsolutions and the function values after adding one or more elements.\nLemma A.4', 'question': 'What is the purpose of Lemma A.6?', 'ans': 'The purpose of Lemma A.6 is to provide a formula for the marginal gain of adding an element to Sw.r.'}, {'input': 'We can write the marginal gain of adding ItoSw.r.t\nFas:\nF(IjS) =n`+1X\nm=1kprojRS(A`\nI)(A`\nVnS)w`+1\nmk2\n2;\nwhere projRS(A`\nI)(A`\nVnS)is the matrix with columns projRS(A`\nI)(a`\nj)for allj2VnS,0otherwise.\n20Proof. SettingB`=A`in Lemma A.5, we get F(IjS) =Pn`+1\nm=1kprojRS(A`\nI)(A`)w`+1\nmk2\n2', 'question': 'What is the formula for the marginal gain of adding itoSw.r.tF?', 'ans': 'F(ijS) =n`+1X m=1kprojRS(a`i)(A`VnS)w`+1mk2/2'}, {'input': 'Given anyi2S,I0=M(i), we have\nFm(I0jL0) =`m(xL0)\x00`m(xI0[L0)\n\x15`m(xL0)\x00`m(xL0\x00[r`m(xL0)]I0\n\x17jL0j+jI0j)\n\x15hr`m(xL0);[r`m(xL0)]I\n\x17jL0j+jI0ji\x00\x17jL0j+k0\n2k[r`m(xL0)]I\n\x17jL0j+jI0jk2\n2\n=k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j\nHence,\nG(SjL)\x14n`+1X\nm=1k[r`m(xL0)]S0k2\n2\n2\x16jL0j+k0=X\ni2S;I0=M(i)k[r`m(xL0)]I0k2\n2\n2\x16jL0j+k0\n=X\ni2S;I0=M(i)\x17jL0j+jI0j\n\x16jL0j+k0k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j=\x17jL0j+k0\n\x16jL0j+k0X\ni2SG(ijL):\nWe thus have \rU;k\x15\x16jU0j+k0\n\x17jU0j+jI0j', 'question': 'What is the purpose of the function F(ijS) described in the given text?', 'ans': 'The function F(ijS) is used to define the matrix projRS(a`i)(A`VnS) with columns projRS(a`i)(a`j) for all j in VnS and 0 otherwise.'}, {'input': '20Proof. SettingB`=A`in Lemma A.5, we get F(IjS) =Pn`+1\nm=1kprojRS(A`\nI)(A`)w`+1\nmk2\n2. Note that\nfor alli2I;j2S,a`\njandRS(a`\ni) are orthogonal, and hence projRS(A`\nI)(a`\nj) = 0, by optimality\nconditions (see proof of Lemma A.4). It follows then that F(IjS) =Pn`+1\nm=1kprojRS(A`\nI)(A`\nVnS)w`+1\nmk2\n2', 'question': 'What is the function of RS(A`I) in the given text?', 'ans': 'RS(A`I) is used to calculate projS(A`I) and projRS(A`I)(y)'}, {'input': 'We can write the marginal gain of adding ItoSw.r.t\nFas:\nF(IjS) =n`+1X\nm=1kprojRS(A`\nI)(A`\nVnS)w`+1\nmk2\n2;\nwhere projRS(A`\nI)(A`\nVnS)is the matrix with columns projRS(A`\nI)(a`\nj)for allj2VnS,0otherwise.\n20Proof. SettingB`=A`in Lemma A.5, we get F(IjS) =Pn`+1\nm=1kprojRS(A`\nI)(A`)w`+1\nmk2\n2', 'question': 'What is the formula for the marginal gain of adding ItoSw.r.t Fas?', 'ans': 'F(IjS) =n`+1X m=1kprojRS(A`I)(A`VnS)w`+1mk2/2'}, {'input': 'Given anyi2S,I0=M(i), we have\nFm(I0jL0) =`m(xL0)\x00`m(xI0[L0)\n\x15`m(xL0)\x00`m(xL0\x00[r`m(xL0)]I0\n\x17jL0j+jI0j)\n\x15hr`m(xL0);[r`m(xL0)]I\n\x17jL0j+jI0ji\x00\x17jL0j+k0\n2k[r`m(xL0)]I\n\x17jL0j+jI0jk2\n2\n=k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j\nHence,\nG(SjL)\x14n`+1X\nm=1k[r`m(xL0)]S0k2\n2\n2\x16jL0j+k0=X\ni2S;I0=M(i)k[r`m(xL0)]I0k2\n2\n2\x16jL0j+k0\n=X\ni2S;I0=M(i)\x17jL0j+jI0j\n\x16jL0j+k0k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j=\x17jL0j+k0\n\x16jL0j+k0X\ni2SG(ijL):\nWe thus have \rU;k\x15\x16jU0j+k0\n\x17jU0j+jI0j', 'question': 'What is the value of F(IjS)?', 'ans': 'F(IjS) = Pn`+1 m=1kprojRS(A`I)(A`VnS)w`+1 mk2 2.'}, {'input': '(4)at the end of Greedy can then be done in O(k\x01n`\x01n`+1)\ntime.\nProof. By Lemma A', 'question': 'What is the time complexity of the algorithm at the end of Greedy?', 'ans': 'The time complexity is O(k*n^2*(n+1)).'}, {'input': 'Gis\rU;k-weakly submodular i\x0b Fsatis\x0ces\n\rU;kF(M(S)jM(L))\x14X\ni2SF(M(i)jM(L));\nfor every two disjoint sets L;S\x12V`, such thatL\x12U;jSj\x14k. We extend the relation established in\n(Elenberg et al', 'question': 'What is the formula to update projS[i(bj)?', 'ans': 'projS[i(bj) =projS(bj) +projRS(b`i)(bj)'}, {'input': '4), using projRS(b`\ni)(bj) =RS(b`\ni)\rS;i(b`\nj) for\nallj2V`nS[i,O(jScj\x01jSj) to update xS[i(b`\nj) =xS(b`\nj) + (1i\x00xS(bi))\rS;i(b`\nj)) (by Lemma A.4)\nfor allj2V`nS[i, andO(n`\x01jSj) to update xS[i(a`\nj) =xS(a`\nj) + (1i\x00xS(bi))\rS;i(a`\nj)) for all\nj2V`', 'question': 'What is the purpose of using projRS(b`i)(bj) =RS(b`i)S;i(b`j) in the given text?', 'ans': 'The purpose of using projRS(b`i)(bj) =RS(b`i)S;i(b`j) in the given text is to update xS[i(b`j)] = xS(b`j) + (1-RS(b`i))xS(b`i)S;i(b`j))'}, {'input': 'Hence, projS[i(y) =projS(y) +projRS(di)(y), where\nprojRS(di)(y) =RS(di)\rS;i(y)', 'question': 'What is the definition of projS(a`j)?', 'ans': 'projS(a`j) is the projection of a`j onto the column space of A`S.'}, {'input': 'Given anyi2S,I0=M(i), we have\nFm(I0jL0) =`m(xL0)\x00`m(xI0[L0)\n\x15`m(xL0)\x00`m(xL0\x00[r`m(xL0)]I0\n\x17jL0j+jI0j)\n\x15hr`m(xL0);[r`m(xL0)]I\n\x17jL0j+jI0ji\x00\x17jL0j+k0\n2k[r`m(xL0)]I\n\x17jL0j+jI0jk2\n2\n=k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j\nHence,\nG(SjL)\x14n`+1X\nm=1k[r`m(xL0)]S0k2\n2\n2\x16jL0j+k0=X\ni2S;I0=M(i)k[r`m(xL0)]I0k2\n2\n2\x16jL0j+k0\n=X\ni2S;I0=M(i)\x17jL0j+jI0j\n\x16jL0j+k0k[r`m(xL0)]I0k2\n2\n2\x17jL0j+jI0j=\x17jL0j+k0\n\x16jL0j+k0X\ni2SG(ijL):\nWe thus have \rU;k\x15\x16jU0j+k0\n\x17jU0j+jI0j', 'question': 'What is the formula for F(ijS)?', 'ans': 'F(ijS) =n`+1X m=1kprojRS(a`i)(A`VnS)w`+1mk22;'}, {'input': '(4)at the end of Greedy can then be done in O(k\x01n`\x01n`+1)\ntime.\nProof. The proof follows from Lemma A.6 and A.4 in the same way as in Proposition A.7.\nProposition 3.2 and Proposition A.7 apply also to Gand ~Grespectively, since jM(i)j=O(1)', 'question': 'What is the time complexity for performing Greedy at the end?', 'ans': 'O(k*n^2)'}, {'input': '2', 'question': 'What is the topic of Section 21B?', 'ans': 'The stronger notion of approximate submodularity satisfied by F and ~F.'}, {'input': '2', 'question': 'What is the topic of Section 21B?', 'ans': 'The topic of Section 21B is a stronger notion of approximate submodularity.'}, {'input': 'Weakly submodular maximization: A set function Fissubmodular if it has diminishing\nmarginal gains: F(ijS)\x15F(ijT) for allS\x12T,i2VnT. We say that Fisnormalized if\nF(;) = 0, and non-decreasing if F(S)\x14F(T) for allS\x12T', 'question': 'What is the definition of a submodular set function?', 'ans': 'A set function F is submodular if it has diminishing marginal gains: F(S∪{i})−F(S) ≥ F(T∪{i})−F(T) for all sets S⊆T and all elements i∉T.'}, {'input': 'Weakly submodular maximization: A set function Fissubmodular if it has diminishing\nmarginal gains: F(ijS)\x15F(ijT) for allS\x12T,i2VnT. We say that Fisnormalized if\nF(;) = 0, and non-decreasing if F(S)\x14F(T) for allS\x12T', 'question': 'What is a set function called if it has diminishing marginal gains?', 'ans': 'A set function is called submodular if it has diminishing marginal gains.'}, {'input': 'If\x00Fis submodular, then\nFis said to be supermodular , i.e.,Fsatis\x0cesF(ijS)\x14F(ijT), for allS\x12T,i2VnT. WhenF\nis both submodular and supermodular, it is said to be modular', 'question': 'What is a function F called when it is both submodular and supermodular?', 'ans': 'When a function F is both submodular and supermodular, it is called modular.'}, {'input': 'B.1 Additional preliminaries\nWe start by reviewing some preliminaries. Recall that a set function Fissubmodular if it has\ndiminishing marginal gains: F(ijS)\x15F(ijT) for allS\x12T,i2VnT. If\x00Fis submodular, then\nFis said to be supermodular , i.e.,Fsatis\x0cesF(ijS)\x14F(ijT), for allS\x12T,i2VnT', 'question': 'What is a modular function?', 'ans': 'When a function is both submodular and supermodular, it is said to be modular.'}, {'input': 'Relaxed notions of submodularity/supermodularity, called weak DR-submodularity/supermodularity ,\nwere introduced in (Lehmann et al., 2006) and (Bian et al., 2017), respectively.\nDe\x0cnition B.1 (Weak DR-sub/supermodularity)', 'question': 'What are weak DR-submodularity and weak DR-supermodularity?', 'ans': 'Weak DR-submodularity and weak DR-supermodularity are relaxed notions of submodularity and supermodularity, respectively, introduced in Lehmann et al., 2006 and Bian et al., 2017.'}, {'input': 'A set function Fis\x0bk-weakly DR-submodular,\nwithk2N+;\x0bk>0, if\nF(ijS)\x15\x0bkF(ijT);for allS\x12T;i2VnT;jTj\x14k:\nSimilarly,Fis\x0ck-weakly DR-supermodular, with k2N+;\x0c > 0, if\nF(ijT)\x15\x0ckF(ijS);for allS\x12T;i2VnT;jTj\x14k:\nWe say that Fis (\x0bk;\x0ck)-weakly DR-modular if it satis\x0ces both properties', 'question': 'What is a set function that is (k, f)-weakly DR-modular?', 'ans': 'A set function that is both k-weakly DR-submodular and f-weakly DR-supermodular.'}, {'input': 'The parameters \x0bk;\x0ckcharacterize how close a set function is to being submodular and supermodular,\nrespectively. If Fis non-decreasing, then \x0bk;\x0ck2[0;1],Fis submodular (supermodular) if and\nonly if\x0bk= 1 (\x0ck= 1) for all k2N+, and modular if and only if both \x0bk=\x0ck= 1 for all\nk2N+', 'question': 'What do the parameters k and k characterize in a set function?', 'ans': 'The parameters k and k characterize how close a set function is to being submodular and supermodular, respectively.'}, {'input': ', 2018, Prop.\n8). This implies that Greedy achieves a (1\x00e\x00\x0b2k\x001)-approximation when Fis\x0b2k\x001-weakly\nDR-submodular.\nA stronger approximation guarantee can be obtained with the notion of total curvature introduced\nin (Sviridenko et al', 'question': 'What is the approximation guarantee achieved by Greedy algorithm when Fis is 2k+1-weakly DR-submodular?', 'ans': 'Greedy achieves a (1e2k+1)-approximation.'}, {'input': 'De\x0cnition B.2 (Total curvature) .Given a set function F, we de\x0cne its total curvature ckwhere\nk2N+, as\nck= 1\x00 min\njSj\x14k;jTj\x14k;i2VnTF(ijS)\nF(ijT):\nNote that if Fhas total curvature ck, thenFis (1\x00ck;1\x00ck)-weakly DR-modular', 'question': 'What is the notion of total curvature?', 'ans': 'The notion of total curvature is a stronger notion of approximate submodularity than weak DR-modularity, introduced in (Sviridenko et al., 2017).'}, {'input': '2 Approximate modularity of reweighted input change\nThe reweighted input change objective Fis closely related to the column subset selection objective\n(the latter is a special case of FwhereW`+1is the identity matrix), whose total curvature was\nshown to be related to the condition number of A`in (Sviridenko et al', 'question': 'What is the relationship between the reweighted input change objective F and the column subset selection objective?', 'ans': 'The column subset selection objective is a special case of F where W+1 is the identity matrix.'}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What is Proposition B.3 about?', 'ans': 'Proposition B.3 shows the relationship between the total curvatures of F and F~ and the condition number of A`W`+1.'}, {'input': 'Both Proposition 3.1 and Proposition 4.1 apply also to the asymmetric variants, using exactly the\nsame proofs.\nProposition A.2. GivenU\x12V;k2N+,~Fis a normalized non-decreasing \rU;k-weakly submodular\nfunction, with\n\rU;k\x15minkzk2=1;kzk0\x14jUj+kkB`zk2\n2\nmaxkzk2=1;kzk0\x14jUj+1kB`zk2\n2:\nProposition A.3', 'question': 'What is Proposition B.3 about?', 'ans': "Proposition B.3 is about a normalized non-decreasing k-weakly DR-submodular function with certain conditions, and its properties if any collection of k+1 columns of B' are linearly independent."}, {'input': 'Proof. We adapt the proof from (Sviridenko et al., 2017, Lemma 6). For all S\x12V;i2VnS, we\nhaveF(ijS) =Pn`+1\nm=1kprojRS(b`\ni)(A`)w`+1\nmk2\n2by Lemma A.5. For all j62S, we have projRS(b`\ni)(a`\nj) =\nRS(b`\ni)RS(b`\ni)>a`\nj\nkRS(b`\ni)k2ifkRS(b`\ni)k>0, and 0 otherwise, by optimality conditions', 'question': 'What is the proof adapted from and which lemma is used in it?', 'ans': 'The proof is adapted from (Sviridenko et al., 2017, Lemma 6).'}, {'input': 'The rest of the\nproposition follows by noting that if any collection of k+ 1 columns of B`are linearly independent,\nthenkRS(b`\ni)k\x15minkzk2\x141;kzk0\x14k+1kB`zk2\n2>0 for anySsuch thatjSj\x14k.\nAs discussed in Section B.1, Proposition B', 'question': 'What is the condition for the rest of the proposition to follow?', 'ans': "If any collection of k+1 columns of B' are linearly independent."}, {'input': '1, Proposition B.3 implies that Greedy achieves a (1\x00e\x00\x0b2k\x001)-\napproximation with F(S), where\x0bkis non-zero if all rows of A`W`+1are linearly independent.\nMoreover, if any k+ 1 columns of A`are linearly independent and all rows of W`+1are linearly\nindependent, then Greedy achieves an \x0bk-approximation', 'question': 'What does Proposition B.3 imply about Greedy?', 'ans': 'Proposition B.3 implies that Greedy achieves a (1e2k1)-approximation with ~F(S), where k is non-zero if all rows of A`W`+1 are linearly independent.'}, {'input': 'The rest of the\nproposition follows by noting that if any collection of k+ 1 columns of B`are linearly independent,\nthenkRS(b`\ni)k\x15minkzk2\x141;kzk0\x14k+1kB`zk2\n2>0 for anySsuch thatjSj\x14k.\nAs discussed in Section B.1, Proposition B', 'question': 'What is the proposition B.4 about?', 'ans': "Proposition B.4 states that if any k+1 columns of B' are linearly independent, then Greedy achieves a k-approximation."}, {'input': 'pdf .\n16A Missing proofs\nRecall that F(S) =kA`W`+1k2\nF\x00min ~W`+12Rn`\x02n`+1kA`W`+1\x00A`\nS~W`+1k2\nF, andG(S) =F(M(S)),\nwhereMmaps each channel to its corresponding columns in A`. We denote by ~F(S) the objective\ncorresponding to the asymmetric formulation introduced in Section 5.1, i.e', 'question': 'What is the definition of F, given the text?', 'ans': 'F is a normalized non-decreasing k-weakly DR-submodular function, with certain constraints on its values and the matrix A.'}, {'input': 'Proof. SettingB`=A`in Lemma A.5, we get\n\x0bk\x15minkzk2=1k(A`W`+1)>zk2\n2\nmaxkzk2=1k(A`W`+1)>zk2\n2\n23To obtain the second lower bound, we note that by Lemma A', 'question': 'What is the purpose of Lemma A.5 in the given text?', 'ans': 'To obtain the first lower bound'}, {'input': 'Proof. SettingB`=A`in Lemma A.5, we get\n\x0bk\x15minkzk2=1k(A`W`+1)>zk2\n2\nmaxkzk2=1k(A`W`+1)>zk2\n2\n23To obtain the second lower bound, we note that by Lemma A', 'question': 'What is the value of k in the given text?', 'ans': 'kRS(a`i)kk2'}, {'input': 'The bound on \x0bkthen follows by noting that kRS(a`\ni)k\x15kRT(a`\ni)kfor allS\x12T. The rest of the\nproposition follows by noting that if any collection of k+ 1 columns of A`are linearly independent,\nthenkRS(a`\ni)k\x15minkzk2\x141;kzk0\x14k+1kA`zk2\n2>0 for anySsuch thatjSj\x14k.\nAs discussed in Section B', 'question': 'What is the bound on k?', 'ans': "The bound on k follows by noting that kRS(a'i)k <= kRT(a'i)k for all S and T."}, {'input': '1, Proposition B.3 implies that Greedy achieves a (1\x00e\x00\x0b2k\x001)-\napproximation with F(S), where\x0bkis non-zero if all rows of A`W`+1are linearly independent.\nMoreover, if any k+ 1 columns of A`are linearly independent and all rows of W`+1are linearly\nindependent, then Greedy achieves an \x0bk-approximation', 'question': 'What does Proposition B.3 imply about Greedy?', 'ans': 'Proposition B.3 implies that Greedy achieves a (1e2k1)-approximation with F(S), where k is non-zero if all rows of A`W`+1 are linearly independent.'}, {'input': 'As discussed in Section B.1, Proposition B.3 implies that Greedy achieves a (1\x00e\x00\x0b2k\x001)-\napproximation with F(S), where\x0bkis non-zero if all rows of A`W`+1are linearly independent', 'question': 'What is the condition for Greedy to achieve a k-approximation?', 'ans': "If any k+1 columns of A' are linearly independent and all rows of W'+1 are linearly independent."}, {'input': 'It is worth noting that if W`+1is identity matrix, i.e., Fis the column subset selection function,\nthen Proposition B.4 implies a stronger result than (Sviridenko et al., 2017, Lemma 6) for some\ncases', 'question': 'What does Proposition B.4 imply when W+1 is an identity matrix?', 'ans': 'Proposition B.4 implies a stronger result than (Sviridenko et al., 2017, Lemma 6) for some cases.'}, {'input': 'A set function Fis\x0bk-weakly DR-submodular,\nwithk2N+;\x0bk>0, if\nF(ijS)\x15\x0bkF(ijT);for allS\x12T;i2VnT;jTj\x14k:\nSimilarly,Fis\x0ck-weakly DR-supermodular, with k2N+;\x0c > 0, if\nF(ijT)\x15\x0ckF(ijS);for allS\x12T;i2VnT;jTj\x14k:\nWe say that Fis (\x0bk;\x0ck)-weakly DR-modular if it satis\x0ces both properties', 'question': 'Under what conditions is F weakly DR-submodular?', 'ans': "F is weakly DR-submodular if any k columns of A' are linearly independent, or if all rows of A' are independent."}, {'input': 'We pre-train LeNet model on MNIST for 200 epochs,\nwith a batch size of 128, using SGD with Nestrov momentum 0 :9 and a \x0cxed learning rate of\n1\x0210\x003. The resulting model achieves 97 :75% top-1 accuracy', 'question': 'What is the batch size used for training LeNet on MNIST?', 'ans': 'The batch size used for training LeNet on MNIST is 128.'}, {'input': 'D Additional results\nD.1 Other channel pruning results\nIn this section, we present additional channel pruning results. We \x0crst consider the same setup\nas in Section 6.2, but we prune four of the convolutional layers of VGG11 (features.0, 8, 11, 18),\ninstead of two', 'question': 'Which convolutional layers of VGG11 were pruned in the additional channel pruning results?', 'ans': 'Four of the convolutional layers were pruned: features.0, 8, 11, and 18.'}, {'input': 'We \x0crst consider the same setup\nas in Section 6.2, but we prune four of the convolutional layers of VGG11 (features.0, 8, 11, 18),\ninstead of two. In these layers, n`= 64;256;512;512, andn`+1= 128;256;512;512, respectively', 'question': 'Which convolutional layers of VGG11 were pruned in the setup considered in this section?', 'ans': 'Four convolutional layers were pruned: features.0, 8, 11, and 18.'}, {'input': '1.\nWe prune the \x0crst two of the eight convolution layers of VGG11, where n`= 64;n`+1= 128 in the\n\x0crst layer,n`= 128;n`+1= 256 in the second one, and the kernel size is 3 \x023, i.e.,rh=rw= 3.\nWe present results with four convolution layers pruned in Appendix D', 'question': 'Which convolutional layers of VGG11 are pruned in this experiment?', 'ans': 'The experiment prunes four of the convolutional layers of VGG11, namely features.0, 8, 11, and 18.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What does Figure 5 show?', 'ans': 'Figure 5 shows the top-1 and top-5 accuracy for different fractions of prunable channels kept.'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which method performs worst in all other settings except for matching the performance of the sequential variants of the method in Figure 5?', 'ans': 'LayerWeightNorm'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which method performs worst in all other settings except when compared to the sequential variants?', 'ans': 'LayerWeightNorm'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which algorithm performs worst in all other settings except for the one mentioned?', 'ans': 'LayerWeightNorm'}, {'input': 'Next we consider the same transfer learning setup as in Section 6.2, but we again prune four of the\nconvolutional layers of VGG11-MNIST (features.0, 8, 11, 18), instead of two. Figure 6 shows the\ntop-1 and top-5 accuracy for di\x0berent fractions of prunable channels kept', 'question': 'Which layers were pruned in the transfer learning setup discussed in this section?', 'ans': 'features.0, 8, 11, 18'}, {'input': '6.3 Transfer learning\nIn this section, we consider a transfer learning setting, where given a large pre-trained network,\nour goal is to obtain a small model on a di\x0berent but related task to the one it was trained on', 'question': 'What transfer learning setup is being considered in this section?', 'ans': 'The same transfer learning setup as in Section 6.2'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What does Figure 6 show?', 'ans': 'Figure 6 shows the top-1 and top-5 accuracy for different fractions of prunable channels kept.'}, {'input': 'All variants of our method\nperform similarly here, and are again among the best performing methods. Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider', 'question': 'Which method performs similarly to all variants of the method and is also among the best performing methods?', 'ans': 'ActGrad'}, {'input': 'Their performance is\nmatched only by ActGrad , which also performs worst in all other settings we consider.\nNote that the results in Figures 5 and 6 are better than their counterparts in Figures 3 and 4,\nwhere we prune less layers', 'question': 'Which model is ActGrad matched by in terms of performance?', 'ans': 'Their performance is matched only by ActGrad.'}, {'input': "Note that the results in Figures 5 and 6 are better than their counterparts in Figures 3 and 4,\nwhere we prune less layers. This is because the current four layers being pruned include a late\n25layer (features.18), which can be pruned almost entirely without a\x0becting the model's accuracy", 'question': 'Why are the results in Figures 5 and 6 better than their counterparts in Figures 3 and 4?', 'ans': "The results in Figures 5 and 6 are better than their counterparts in Figures 3 and 4 because the current four layers being pruned include a late 25 layer (features.18), which can be pruned almost entirely without affecting the model's accuracy."}, {'input': "18), which can be pruned almost entirely without a\x0becting the model's accuracy.\nHence, given a global budget, the per-layer budget selection assigns most of the budget to the \x0crst\nthree layers, which yield better results than when we are forced to prune a large fraction of early\nlayers", 'question': 'Which layers are assigned most of the budget in per-layer budget selection?', 'ans': 'The first three layers.'}, {'input': 'We further highlight the e\x0bect of per-layer budget selection on performance in Appendix F.\nReweighting again helps all methods in both pruning tasks considered here', 'question': 'What is highlighted in Appendix F?', 'ans': 'The effect of per-layer budget selection on performance is highlighted in Appendix F.'}, {'input': 'Figure 5: Top-1 and Top-5 Accuracy of di\x0berent channel pruning methods on CIFAR10, after\npruning four convolution layers in VGG11 model, with di\x0berent fractions of remaining channels (in\nlog-scale), with (left) and without (right) reweighting', 'question': 'What does Figure 5 show for different channel pruning methods on CIFAR10?', 'ans': 'Figure 5 shows the Top-1 and Top-5 Accuracy of different channel pruning methods on CIFAR10, after pruning four convolution layers in VGG11 model, with different fractions of remaining channels (in log-scale), with and without reweighting.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What does Figure 6 show?', 'ans': 'Figure 6 shows the Top-1 and Top-5 accuracy of different channel pruning methods on MNIST, after pruning four convolution layers in VGG11-MNIST model, with different fractions of remaining channels (in log-scale), with and without reweighting.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What do Figures 7, 8, 9, and 10 report?', 'ans': 'The top-5 accuracy results of the experiments presented in Section 6.'}, {'input': '4 Pruning regular regions of neurons\nIn this section, we discuss how to adapt our approach to pruning regular regions of neurons. This\nis easily achieved by mapping any set of regular regions to the corresponding set of neurons, then\napplying the same method in Section 3', 'question': 'What is the focus of this section?', 'ans': 'The focus of this section is the effect of fine-tuning.'}, {'input': 'To that end, we report in Figures 11, 12, 13, 14,\n15, and 16, the top-1 and top-5 accuracy results of all the pruning tasks considered in Section 6 and\nAppendix D.1 after \x0cne-tuning.\nWe \x0cne-tune for 10 epochs in all MNIST experiments, and for 20 epochs for all CIFAR-10 experiments', 'question': 'What are the figures that report the top-1 and top-5 accuracy results of all the pruning tasks?', 'ans': 'Figures 11, 12, 13, 14, 15, and 16'}, {'input': '•Fine-tuning boosts performance even more than reweighting, as illustrated in Appendix E.\nThough reweighting often still helps even when \x0cne-tuning is used. Hence, both \x0cne-tuning\nand reweighting should be used when possible, i.e., in settings where data is not limited', 'question': 'What is the purpose of fine-tuning in the experiments?', 'ans': 'The purpose of fine-tuning is to provide a significant boost in performance to all methods.'}, {'input': '•Fine-tuning boosts performance even more than reweighting, as illustrated in Appendix E.\nThough reweighting often still helps even when \x0cne-tuning is used. Hence, both \x0cne-tuning\nand reweighting should be used when possible, i.e., in settings where data is not limited', 'question': 'What is the effect of fine-tuning on the performance of the methods?', 'ans': 'Fine-tuning provides a significant boost in performance to all methods.'}, {'input': 'We pre-train LeNet model on MNIST for 200 epochs,\nwith a batch size of 128, using SGD with Nestrov momentum 0 :9 and a \x0cxed learning rate of\n1\x0210\x003. The resulting model achieves 97 :75% top-1 accuracy', 'question': 'What is Figure 7 showing in regards to the LeNet model on MNIST?', 'ans': 'Figure 7 shows the top-5 accuracy of different neuron pruning methods on MNIST, after pruning two linear layers in the LeNet model, with different fractions of remaining neurons, with and without reweighting.'}, {'input': '2', 'question': 'What does Figure 8 show?', 'ans': 'Figure 8 shows the top-5 accuracy of different neuron pruning methods on CIFAR10, after pruning two linear layers in VGG11 model, with different fractions of remaining neurons (in log-scale), with and without reweighting.'}, {'input': '2', 'question': 'What does Figure 9 show?', 'ans': 'Figure 9 shows the top-5 accuracy of different channel pruning methods on CIFAR10, after pruning two convolution layers in VGG11 model, with different fractions of remaining channels (in log-scale), with and without reweighting.'}, {'input': '2', 'question': 'What does Figure 10 show?', 'ans': 'Figure 10 shows the top-5 accuracy of different channel pruning methods on MNIST, after pruning two convolution layers in VGG11-MNIST model, with different fractions of remaining channels (in log-scale), with and without reweighting.'}, {'input': '2), we report results with reweighting applied to\nall pruning methods, or none of them. Though, we will focus our analysis on the more interesting\nresults with reweighting, with the plots without reweighting mostly serving as a demonstration of\nthe bene\x0ct of reweighting', 'question': 'How do the results of the methods compare when reweighting is used?', 'ans': 'The methods have slightly lower accuracy when reweighting is used, as shown in Figure 13, for fractions larger than 0.05.'}, {'input': 'When both \x0cne-tuning and reweighting are used, all methods\nperform fairly similarly, except WeightNorm which consistently performs worst, and random\npruning methods which sometimes perform worst (see Figures 13 and 14)', 'question': 'Which method consistently performs worst when both fine-tuning and reweighting are used?', 'ans': 'WeightNorm'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the focus of the study in this section?', 'ans': 'The focus of the study in this section is the effect of per-layer budget selection on accuracy.'}, {'input': "Note that the results in Figures 5 and 6 are better than their counterparts in Figures 3 and 4,\nwhere we prune less layers. This is because the current four layers being pruned include a late\n25layer (features.18), which can be pruned almost entirely without a\x0becting the model's accuracy", 'question': 'Which layers are pruned in the modified setup described in the text?', 'ans': 'The first and second to last layers of VGG11 are pruned.'}, {'input': 'Since the second to last layer in VGG11\n(features.22) has little e\x0bect on accuracy when pruned, we expect the choice of how the global budget\nis distributed on the two layers to have a signi\x0ccant impact on performance', 'question': 'What is the expected impact of the distribution of the global budget on the two layers in VGG11?', 'ans': 'It is expected to have a significant impact on performance.'}, {'input': 'Figure 17 shows the\ntop-1 accuracy for di\x0berent fractions of prunable channels kept, when the per-layer budget selection\nfrom Section 5.2 is used, while Figure 18 shows the results when equal fractions of channels kept\nare used in each layer', 'question': 'What do Figures 17 and 18 show?', 'ans': 'Figure 17 shows the top-1 accuracy for different fractions of prunable channels kept using per-layer budget selection, while Figure 18 shows the results when equal fractions of channels kept are used in each layer.'}, {'input': 'As expected, all layerwise methods perform much more poorly with equal\n27Figure 11: Top-1 and Top-5 Accuracy of di\x0berent neuron pruning methods on MNIST, after pruning\ntwo linear layers in LeNet model, and \x0cne-tuning for 10 epochs, with di\x0berent fractions of remaining\nneurons (in log-scale), with (left) and without (right) reweighting', 'question': 'What is the performance of layerwise methods on MNIST after pruning two linear layers in LeNet model and fine-tuning for 10 epochs?', 'ans': 'All layerwise methods perform much more poorly with equal fractions of remaining neurons.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the purpose of the experiment described in the text?', 'ans': 'The purpose of the experiment is to compare the Top-1 and Top-5 accuracy of different neuron pruning methods on CIFAR10 after pruning two linear layers in VGG11 model and fine-tuning for 20 epochs, with different fractions of remaining neurons, with and without reweighting.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the purpose of the experiment described in the text?', 'ans': 'The purpose of the experiment is to compare the Top-1 and Top-5 accuracy of different channel pruning methods on CIFAR10 after pruning two convolution layers in VGG11 model and fine-tuning for 20 epochs, with different fractions of remaining channels, with and without reweighting.'}, {'input': 'per-layer fractions, both with and without \x0cne-tuning. Though, the di\x0berence is less drastic when\n\x0cne-tuning is used', 'question': 'What is the difference between per-layer fractions with and without fine-tuning?', 'ans': 'The difference is less drastic when fine-tuning is used.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the purpose of the experiment described in Figure 14?', 'ans': 'The purpose of the experiment is to compare the Top-1 and Top-5 accuracy of different channel pruning methods on CIFAR10, after pruning four convolution layers in VGG11 model, and fine-tuning for 20 epochs, with different fractions of remaining channels, with and without reweighting.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the purpose of the experiment described in the text?', 'ans': 'The purpose of the experiment is to compare the Top-1 and Top-5 accuracy of different channel pruning methods on MNIST after pruning two convolution layers in VGG11-MNIST model, and fine-tuning for 10 epochs, with different fractions of remaining channels, with and without reweighting.'}, {'input': 'D.2 Top-5 Accuracy results of Section 6\nFor completeness, we report in Figures 7, 8, 9, and 10 the top-5 accuracy results of the experiments\npresented in Section 6.\nE E\x0bect of \x0cne-tuning\nIn this section, we study the e\x0bect of \x0cne-tuning', 'question': 'What is the purpose of the experiment described in the text?', 'ans': 'The purpose of the experiment is to compare the Top-1 and Top-5 accuracy of different channel pruning methods on MNIST after pruning four convolution layers in VGG11-MNIST model, and fine-tuning for 10 epochs, with different fractions of remaining channels, with and without reweighting.'}, {'input': '2', 'question': 'What is the purpose of Figure 17?', 'ans': 'The purpose of Figure 17 is to show the top-1 accuracy of different channel pruning methods on CIFAR10, after pruning the first and second to last convolution layers in VGG11 model, with different fractions of remaining channels, with and without reweighting, with per-layer fractions selected using the selection method discussed in Section 5.'}, {'input': 'Figure 17 shows the\ntop-1 accuracy for di\x0berent fractions of prunable channels kept, when the per-layer budget selection\nfrom Section 5.2 is used, while Figure 18 shows the results when equal fractions of channels kept\nare used in each layer', 'question': 'What does Figure 18 show?', 'ans': 'Figure 18 shows the top-1 accuracy of different channel pruning methods on CIFAR10, after pruning the first and second to last convolution layers in VGG11 model, with different fractions of remaining channels, with and without reweighting, with equal per-layer fractions.'}