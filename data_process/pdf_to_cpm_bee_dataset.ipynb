{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LangChain加载PDF，并对文本进行切割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"../data/sentic-gcn.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1120, which is longer than the specified 500\n",
      "Created a chunk of size 1758, which is longer than the specified 500\n",
      "Created a chunk of size 516, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for doc in docs:\n",
    "    texts.extend(text_splitter.split_text(doc.page_content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过langchain的Prompt和OpenAI，将Text转换为CPM-BEE可用的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain import PromptTemplate, LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = ResponseSchema(name=\"input\",\n",
    "                              description=\"the text inputed\")\n",
    "question_schema = ResponseSchema(name=\"question\",\n",
    "                                 description=\"the question generated by llms\")\n",
    "answer_schema = ResponseSchema(name=\"<ans>\",\n",
    "                               description=\"the anwser generated by llms\")\n",
    "\n",
    "response_schemas = [input_schema, \n",
    "                    question_schema,\n",
    "                    answer_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"input\": string  // the text inputed\n",
      "\t\"question\": string  // the question generated by llms\n",
      "\t\"<ans>\": string  // the anwser generated by llms\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Ask {numbers} and generate the answer base on the text \\\n",
    "that is delimited by triple backticks. \\\n",
    "text: ```{text}```\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"numbers\",'text',\"format_instructions\"],\n",
    "    template=template_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = []\n",
    "for text in texts:\n",
    "    response = llm_chain.run({\n",
    "        \"numbers\": 5,\n",
    "        \"text\": text,\n",
    "        \"format_instructions\": format_instructions\n",
    "    })\n",
    "    try:\n",
    "        output_dict = output_parser.parse(response)\n",
    "        qa.append(output_dict)\n",
    "    except:\n",
    "        print(f\"patse error: {response}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save qa to json file\n",
    "import json\n",
    "with open(\"../data/sentic-gcn.json\", \"w\") as f:\n",
    "    json.dump(qa, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
